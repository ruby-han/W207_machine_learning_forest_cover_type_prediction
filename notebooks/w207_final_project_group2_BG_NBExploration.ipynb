{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "# import kaggle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "# Learning libs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Producing Decision Tree diagrams\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "from subprocess import call\n",
    "\n",
    "# For other \n",
    "import copy\n",
    "from textwrap import wrap\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "# Import Train and Dev data from processed data\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv').set_index('Id')\n",
    "train_labels = pd.read_csv('../data/processed/train_labels.csv').set_index('Id')\n",
    "dev_data = pd.read_csv('../data/processed/dev_data.csv').set_index('Id')\n",
    "dev_labels = pd.read_csv('../data/processed/dev_labels.csv').set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_data.head()\n",
    "#dev_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4429</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>215.564376</td>\n",
       "      <td>192</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>205.548048</td>\n",
       "      <td>201</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>384.480169</td>\n",
       "      <td>324</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>313.180459</td>\n",
       "      <td>309</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10572</th>\n",
       "      <td>296.325834</td>\n",
       "      <td>295</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11892</th>\n",
       "      <td>212.084889</td>\n",
       "      <td>212</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total_Distance_To_Hydrology  Horizontal_Distance_To_Hydrology  \\\n",
       "Id                                                                     \n",
       "4429                      0.000000                                 0   \n",
       "12400                     0.000000                                 0   \n",
       "4648                    215.564376                               192   \n",
       "5954                      0.000000                                 0   \n",
       "2947                      0.000000                                 0   \n",
       "9253                    205.548048                               201   \n",
       "3472                    384.480169                               324   \n",
       "3079                    313.180459                               309   \n",
       "10572                   296.325834                               295   \n",
       "11892                   212.084889                               212   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  \n",
       "Id                                     \n",
       "4429                                0  \n",
       "12400                               0  \n",
       "4648                               98  \n",
       "5954                                0  \n",
       "2947                                0  \n",
       "9253                               43  \n",
       "3472                              207  \n",
       "3079                               51  \n",
       "10572                              28  \n",
       "11892                               6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing final column that includes CoverType\n",
    "train = train_data.copy()\n",
    "dev = dev_data.copy()\n",
    "\n",
    "# Total_Distance_to_Hydrology\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "train['Total_Distance_To_Hydrology'] = np.sqrt(train_data[\"Horizontal_Distance_To_Hydrology\"]**2 + train_data['Vertical_Distance_To_Hydrology']**2)\n",
    "dev['Total_Distance_To_Hydrology'] = np.sqrt(dev_data[\"Horizontal_Distance_To_Hydrology\"]**2 + dev_data['Vertical_Distance_To_Hydrology']**2)\n",
    "train[[\"Total_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
      "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
      "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
      "       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n",
      "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
      "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
      "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
      "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
      "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
      "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
      "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
      "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
      "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
      "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
      "       'Soil_Type39', 'Soil_Type40', 'Total_Distance_To_Hydrology'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <th>Wilderness_Area4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3024 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Wilderness_Area1  Wilderness_Area2  Wilderness_Area3  Wilderness_Area4\n",
       "Id                                                                           \n",
       "5415                  0                 0                 0                 1\n",
       "6658                  0                 0                 1                 0\n",
       "3551                  0                 0                 0                 1\n",
       "5155                  0                 0                 0                 1\n",
       "10748                 0                 0                 1                 0\n",
       "...                 ...               ...               ...               ...\n",
       "13124                 0                 0                 0                 1\n",
       "3265                  0                 0                 0                 1\n",
       "9846                  0                 0                 1                 0\n",
       "10800                 0                 0                 1                 0\n",
       "2733                  0                 0                 1                 0\n",
       "\n",
       "[3024 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all columns\n",
    "print(dev.columns)\n",
    "dev.iloc[:,10:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "### Normalizing continuous features (GaussianNB)\n",
    "\n",
    "First, let's look at a Confusion Matrix after normalizing and using all the features. We want to differentiate between those that get confused most often.\n",
    "\n",
    "First, I normalized all columns that have numerical data. Columns with binary data stayed as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.269012</td>\n",
       "      <td>-1.260394</td>\n",
       "      <td>-0.535135</td>\n",
       "      <td>-0.672762</td>\n",
       "      <td>-1.059503</td>\n",
       "      <td>-1.096632</td>\n",
       "      <td>-0.048712</td>\n",
       "      <td>-0.173892</td>\n",
       "      <td>0.101598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.463055</td>\n",
       "      <td>-0.343459</td>\n",
       "      <td>-0.771915</td>\n",
       "      <td>-0.593370</td>\n",
       "      <td>-0.787843</td>\n",
       "      <td>-1.096632</td>\n",
       "      <td>0.796328</td>\n",
       "      <td>0.571621</td>\n",
       "      <td>-0.225332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.096529</td>\n",
       "      <td>-0.434245</td>\n",
       "      <td>1.122330</td>\n",
       "      <td>-0.636469</td>\n",
       "      <td>-0.073480</td>\n",
       "      <td>-0.095667</td>\n",
       "      <td>1.316353</td>\n",
       "      <td>-0.787845</td>\n",
       "      <td>-1.685622</td>\n",
       "      <td>192.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.969562</td>\n",
       "      <td>-0.588580</td>\n",
       "      <td>-0.416745</td>\n",
       "      <td>-0.951770</td>\n",
       "      <td>-0.885714</td>\n",
       "      <td>-1.096632</td>\n",
       "      <td>0.893833</td>\n",
       "      <td>0.045376</td>\n",
       "      <td>-0.574058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.920614</td>\n",
       "      <td>1.726454</td>\n",
       "      <td>-0.416745</td>\n",
       "      <td>-1.162726</td>\n",
       "      <td>-0.870164</td>\n",
       "      <td>-1.096632</td>\n",
       "      <td>-0.601239</td>\n",
       "      <td>-0.042331</td>\n",
       "      <td>0.581096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12091</th>\n",
       "      <td>0.072523</td>\n",
       "      <td>-0.143730</td>\n",
       "      <td>-0.061574</td>\n",
       "      <td>0.751007</td>\n",
       "      <td>0.443313</td>\n",
       "      <td>1.433698</td>\n",
       "      <td>0.958836</td>\n",
       "      <td>0.659329</td>\n",
       "      <td>-0.421491</td>\n",
       "      <td>525.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>0.388742</td>\n",
       "      <td>-1.169608</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.610699</td>\n",
       "      <td>-0.654624</td>\n",
       "      <td>-0.048712</td>\n",
       "      <td>-0.743991</td>\n",
       "      <td>-0.268923</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12093</th>\n",
       "      <td>-1.618769</td>\n",
       "      <td>0.537162</td>\n",
       "      <td>0.530378</td>\n",
       "      <td>-0.491294</td>\n",
       "      <td>-1.074138</td>\n",
       "      <td>-1.096632</td>\n",
       "      <td>-0.601239</td>\n",
       "      <td>1.536404</td>\n",
       "      <td>1.169571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12094</th>\n",
       "      <td>-0.792288</td>\n",
       "      <td>-0.697523</td>\n",
       "      <td>0.411988</td>\n",
       "      <td>-0.799034</td>\n",
       "      <td>-0.176838</td>\n",
       "      <td>1.186007</td>\n",
       "      <td>0.958836</td>\n",
       "      <td>-0.831699</td>\n",
       "      <td>-1.249715</td>\n",
       "      <td>484.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12095</th>\n",
       "      <td>1.689552</td>\n",
       "      <td>1.790004</td>\n",
       "      <td>-0.653525</td>\n",
       "      <td>-0.439878</td>\n",
       "      <td>0.063722</td>\n",
       "      <td>1.639232</td>\n",
       "      <td>-0.341226</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.472120</td>\n",
       "      <td>552.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12096 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -1.269012 -1.260394 -0.535135 -0.672762 -1.059503 -1.096632 -0.048712   \n",
       "1     -1.463055 -0.343459 -0.771915 -0.593370 -0.787843 -1.096632  0.796328   \n",
       "2     -1.096529 -0.434245  1.122330 -0.636469 -0.073480 -0.095667  1.316353   \n",
       "3     -0.969562 -0.588580 -0.416745 -0.951770 -0.885714 -1.096632  0.893833   \n",
       "4     -1.920614  1.726454 -0.416745 -1.162726 -0.870164 -1.096632 -0.601239   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12091  0.072523 -0.143730 -0.061574  0.751007  0.443313  1.433698  0.958836   \n",
       "12092  0.388742 -1.169608  0.056817  0.187700  0.610699 -0.654624 -0.048712   \n",
       "12093 -1.618769  0.537162  0.530378 -0.491294 -1.074138 -1.096632 -0.601239   \n",
       "12094 -0.792288 -0.697523  0.411988 -0.799034 -0.176838  1.186007  0.958836   \n",
       "12095  1.689552  1.790004 -0.653525 -0.439878  0.063722  1.639232 -0.341226   \n",
       "\n",
       "             7         8      9   ...   45   46   47   48   49   50   51   52  \\\n",
       "0     -0.173892  0.101598    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.571621 -0.225332    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2     -0.787845 -1.685622  192.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.045376 -0.574058    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4     -0.042331  0.581096    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "12091  0.659329 -0.421491  525.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "12092 -0.743991 -0.268923   95.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "12093  1.536404  1.169571    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "12094 -0.831699 -1.249715  484.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "12095  0.089230  0.472120  552.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        53   54  \n",
       "0      0.0  0.0  \n",
       "1      0.0  0.0  \n",
       "2      0.0  0.0  \n",
       "3      0.0  0.0  \n",
       "4      0.0  0.0  \n",
       "...    ...  ...  \n",
       "12091  0.0  0.0  \n",
       "12092  0.0  0.0  \n",
       "12093  0.0  0.0  \n",
       "12094  0.0  0.0  \n",
       "12095  0.0  1.0  \n",
       "\n",
       "[12096 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ColumnTransformer only transforms select columns\n",
    "# StandardScalar normalizes the data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features to normalize\n",
    "feature_cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Roadways\",\n",
    "                \"Horizontal_Distance_To_Fire_Points\", \"Total_Distance_To_Hydrology\",\n",
    "                \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('features', StandardScaler(), feature_cols)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Transform features\n",
    "nX_train = ct.fit_transform(train)\n",
    "nX_dev = ct.transform(dev)\n",
    "\n",
    "# Capture labels\n",
    "y_train = train_labels.Cover_Type\n",
    "y_dev = dev_labels.Cover_Type\n",
    "\n",
    "# Sanity Check!\n",
    "pd.DataFrame(nX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      " [[ 55  23  29 109   1 145  74]\n",
      " [ 69  31  54  55   0 158  67]\n",
      " [ 12  11 103  68   1 190  38]\n",
      " [ 14   2  77 264   0  77  21]\n",
      " [ 16  11  82 104   0 176  36]\n",
      " [  6  11  84 109   0 194  24]\n",
      " [ 38  20  26  86   0 124 129]] \n",
      "\n",
      "Accuracy: 0.2566\n"
     ]
    }
   ],
   "source": [
    "# This model using all features, with continuous vars normalized\n",
    "\n",
    "# Define model\n",
    "model = GaussianNB(var_smoothing=0.001)\n",
    "model.fit(nX_train, y_train)\n",
    "\n",
    "# Predict the class labels for the provided data\n",
    "pred_labels = model.predict(nX_dev)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_dev, pred_labels)\n",
    "print('Confusion Matrix: \\n\\n', cm, \"\\n\")\n",
    "print(\"Accuracy: %3.4f\" %accuracy_score(y_dev, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping all binary features (GaussianNB)\n",
    "\n",
    "The accuracy of the previous model using all features performed very poorly. Let's first drop all binary features, namely Soil Types and Wilderness Areas, as well as Horizontal and Vertical Distance to Hydrology (since we now have the Total Distance to Hydrology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Train Accuracy: 0.6025132275132276\n",
      "Initial Train F1 score: 0.5911853094372181 \n",
      "\n",
      "Initial Dev Accuracy: 0.6025132275132276\n",
      "Initial Dev F1 score: 0.5909465797445023 \n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[239  54   0   0  37   4 102]\n",
      " [119 164  13   1  95  17  25]\n",
      " [  0   6 164  93  54 106   0]\n",
      " [  0   0  28 378   0  49   0]\n",
      " [  3  80   6   0 305  31   0]\n",
      " [  0  14 101  65  43 205   0]\n",
      " [ 52   1   0   0   3   0 367]] \n",
      "\n",
      "Mean sigma: 0.786993279128291 \n",
      "\n",
      "Train Accuracy for constant sigma = 0.79 : 0.4582506613756614\n",
      "Train F1 score for constant sigma =  0.79 : 0.44551342430003094 \n",
      "\n",
      "Dev Accuracy for constant sigma = 0.79 : 0.44775132275132273\n",
      "Dev F1 score for constant sigma =  0.79 : 0.431144389067487 \n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[151  64   3   0  96  15 107]\n",
      " [ 91 128  10   5 112  23  65]\n",
      " [  0  23  66 122  74 138   0]\n",
      " [  0   3  31 348   6  67   0]\n",
      " [ 30  86   9  36 235  26   3]\n",
      " [  0  29  39  96  53 211   0]\n",
      " [148   2   0   0  56   2 215]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes - Continuous Data Only\n",
    "# -----------------------------------------------------------------------------\n",
    "def naive_bayes_con():\n",
    "    # naive bayes cleaning/training/testing\n",
    "    \n",
    "    # Features to be used in model\n",
    "#     feature_cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Total_Distance_To_Hydrology\",\n",
    "#                   \"Horizontal_Distance_To_Roadways\",\n",
    "#                   \"Horizontal_Distance_To_Fire_Points\", \"Hillshade_9am\",\n",
    "#                   \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "    \n",
    "    # Drop features\n",
    "    drop = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "           'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "           'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "           'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "           'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "           'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "           'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "           'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "           'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "           'Soil_Type39', 'Soil_Type40', \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"Vertical_Distance_To_Hydrology\", 'Wilderness_Area1', 'Wilderness_Area2',\n",
    "           'Wilderness_Area3', 'Wilderness_Area4']\n",
    "\n",
    "    nX_train = train.drop(drop, axis = 1)\n",
    "    nX_dev = dev.drop(drop, axis = 1)\n",
    "    \n",
    "    # Using StandardScalar() to normalize data\n",
    "    scaler = StandardScaler()\n",
    "    nX_train = scaler.fit_transform(nX_train)\n",
    "    nX_dev = scaler.transform(nX_dev)\n",
    "\n",
    "    # Create new dataframes with normalized values of data\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    # Generating initial model\n",
    "    #smoothing_list = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    #for num in smoothing_list:\n",
    "\n",
    "    model = GaussianNB(var_smoothing=0.00001)\n",
    "    model.fit(nX_train, y_train)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "\n",
    "    # Print the results\n",
    "    print('\\nInitial Train Accuracy:', model.score(nX_train, y_train))\n",
    "    print('Initial Train F1 score:', metrics.f1_score(y_train, pred_y_train, average = 'weighted'), '\\n')\n",
    "    print('Initial Dev Accuracy:', model.score(nX_dev, y_dev))\n",
    "    print('Initial Dev F1 score:', metrics.f1_score(y_dev, pred_y_dev, average = 'weighted'), '\\n')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(dev_labels, pred_y_dev)\n",
    "    print('Confusion Matrix: \\n\\n', cm, \"\\n\")\n",
    "    \n",
    "    # Adjusting sigma to test results\n",
    "    print(\"Mean sigma:\", np.mean(model.sigma_), '\\n')\n",
    "\n",
    "    sigma = 0.79\n",
    "    model.sigma_ = np.full((7,nX_train.shape[1]), sigma)\n",
    "    train_accuracy = model.score(nX_train, y_train)\n",
    "    dev_accuracy = model.score(nX_dev, y_dev)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "    train_f1_score = metrics.f1_score(y_train, pred_y_train, average = 'weighted')\n",
    "    dev_f1_score = metrics.f1_score(y_dev, pred_y_dev, average = 'weighted')\n",
    "\n",
    "    # Print the results\n",
    "    print('Train Accuracy for constant sigma =', sigma, ':', train_accuracy)\n",
    "    print('Train F1 score for constant sigma = ', sigma, ':', train_f1_score, '\\n')\n",
    "    print('Dev Accuracy for constant sigma =', sigma, ':', dev_accuracy)\n",
    "    print('Dev F1 score for constant sigma = ', sigma, ':', dev_f1_score, '\\n\\n')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(dev_labels, pred_y_dev)\n",
    "    print('Confusion Matrix: \\n\\n', cm, \"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "naive_bayes_con()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that using only the continuous data worked a lot better than before. However, I am sure there is a way to transform the binary data so that including the transformed columns will improve the model. Or, as I will attempt below, we may want to use the output of two models as input to a final model. \n",
    "\n",
    "Before this, however, let's drop the continuous features and model only the binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the continuous features (GaussianNB)\n",
    "\n",
    "Now let's try dropping the continuous features. This way, the confusion matrix may help us determine specifically which classes are being mixed up based on soil type and wilderness area alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
      "       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
      "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
      "       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
      "       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
      "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
      "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
      "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
      "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
      "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
      "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.45849867724867727\n",
      "Initial Train F1 score: 0.38043065166553397 \n",
      "\n",
      "Initial Dev Accuracy: 0.4679232804232804\n",
      "Initial Dev F1 score: 0.38680087268104096 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[ 65   7   4   0 197   1 162]\n",
      " [ 31  48  27   6 213   1 108]\n",
      " [  0   0 180 242   0   0   1]\n",
      " [  0   0   5 450   0   0   0]\n",
      " [  0   2  95   0 290   2  36]\n",
      " [  0   0 141 233  16  13  25]\n",
      " [  6   0   3   0  45   0 369]] \n",
      "\n",
      "Mean sigma: 0.028796458707471776 \n",
      "\n",
      "Train Accuracy for constant sigma = 0.028 : 0.5064484126984127\n",
      "Train F1 score for constant sigma =  0.028 : 0.46063746050931803 \n",
      "\n",
      "Dev Accuracy for constant sigma = 0.028 : 0.521494708994709\n",
      "Dev F1 score for constant sigma =  0.028 : 0.4774876773202978 \n",
      "\n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[142  99   0   0 140   0  55]\n",
      " [ 40 187   0   2 171  14  20]\n",
      " [  0   0   0 129 141 143  10]\n",
      " [  0   0   0 419   0  36   0]\n",
      " [ 13  64   0   0 334  12   2]\n",
      " [  0   0   0  97  98 224   9]\n",
      " [ 76  31   0   0  45   0 271]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes - Binary Data Only using GAUSSIANNB\n",
    "# -----------------------------------------------------------------------------\n",
    "def naive_bayes_bin():\n",
    "    # naive bayes cleaning/training/testing\n",
    "    \n",
    "    # Drop features\n",
    "    drop = [\"Elevation\", \"Aspect\", \"Slope\", \"Total_Distance_To_Hydrology\",\n",
    "            \"Horizontal_Distance_To_Roadways\", \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Fire_Points\",\n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "\n",
    "    nX_train = train.drop(drop, axis = 1)\n",
    "    nX_dev = dev.drop(drop, axis = 1)\n",
    "    print(nX_train.columns, \"\\n\")\n",
    "\n",
    "    # Create new dataframes with normalized values of data\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    # Generating initial model\n",
    "    #smoothing_list = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    #for num in smoothing_list:\n",
    "\n",
    "    model = GaussianNB(var_smoothing=0.00001)\n",
    "    model.fit(nX_train, y_train)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "\n",
    "    # Print the results\n",
    "    print('\\nInitial Train Accuracy:', model.score(nX_train, y_train))\n",
    "    print('Initial Train F1 score:', metrics.f1_score(y_train, pred_y_train, average = 'weighted'), '\\n')\n",
    "    print('Initial Dev Accuracy:', model.score(nX_dev, y_dev))\n",
    "    print('Initial Dev F1 score:', metrics.f1_score(y_dev, pred_y_dev, average = 'weighted'), '\\n')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_dev, pred_y_dev)\n",
    "    print('Confusion Matrix for Dev: \\n\\n', cm, \"\\n\")\n",
    "    \n",
    "    # Adjusting sigma to test results\n",
    "    print(\"Mean sigma:\", np.mean(model.sigma_), '\\n')\n",
    "\n",
    "    sigma = 0.028\n",
    "    model.sigma_ = np.full((7,nX_train.shape[1]), sigma)\n",
    "    train_accuracy = model.score(nX_train, y_train)\n",
    "    dev_accuracy = model.score(nX_dev, y_dev)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "    train_f1_score = metrics.f1_score(y_train, pred_y_train, average = 'weighted')\n",
    "    dev_f1_score = metrics.f1_score(y_dev, pred_y_dev, average = 'weighted')\n",
    "    \n",
    "\n",
    "    # Print the results\n",
    "    print('Train Accuracy for constant sigma =', sigma, ':', train_accuracy)\n",
    "    print('Train F1 score for constant sigma = ', sigma, ':', train_f1_score, '\\n')\n",
    "    print('Dev Accuracy for constant sigma =', sigma, ':', dev_accuracy)\n",
    "    print('Dev F1 score for constant sigma = ', sigma, ':', dev_f1_score, '\\n\\n')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_dev, pred_y_dev)\n",
    "    print('Confusion Matrix for Dev: \\n\\n', cm, \"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "naive_bayes_bin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the continuous features (BernoulliNB)\n",
    "\n",
    "Evidently, using a BernoulliNB model would be more appropriate in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
      "       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
      "       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n",
      "       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n",
      "       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n",
      "       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
      "       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n",
      "       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n",
      "       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n",
      "       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n",
      "       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.5794477513227513\n",
      "Initial Train F1 score: 0.5748263952129261 \n",
      "\n",
      "Initial Dev Accuracy: 0.5846560846560847\n",
      "Initial Dev F1 score: 0.5806767674313877 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[172 132   3   0  79   0  50]\n",
      " [ 61 239   3   2 112  15   2]\n",
      " [  0   1 163 103  13 143   0]\n",
      " [  0   0  58 361   0  36   0]\n",
      " [ 39  80  31   0 263  12   0]\n",
      " [  5  14  66  64  47 232   0]\n",
      " [ 30  28   3   0  24   0 338]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Binary Data Only using BERNOULLINB\n",
    "# -----------------------------------------------------------------------------\n",
    "def naive_bayes_bern():\n",
    "    # naive bayes cleaning/training/testing\n",
    "    \n",
    "    # Drop features\n",
    "    drop = [\"Elevation\", \"Aspect\", \"Slope\", \"Total_Distance_To_Hydrology\",\n",
    "            \"Horizontal_Distance_To_Roadways\", \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Fire_Points\",\n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "\n",
    "    nX_train = train.drop(drop, axis = 1)\n",
    "    nX_dev = dev.drop(drop, axis = 1)\n",
    "    print(nX_train.columns, \"\\n\")\n",
    "\n",
    "    # Create new dataframes with normalized values of data\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    # Generating initial model\n",
    "    #smoothing_list = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    #for num in smoothing_list:\n",
    "\n",
    "    model = BernoulliNB(alpha=0.00001)\n",
    "    model.fit(nX_train, y_train)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "\n",
    "    # Print the results\n",
    "    print('\\nInitial Train Accuracy:', model.score(nX_train, y_train))\n",
    "    print('Initial Train F1 score:', metrics.f1_score(y_train, pred_y_train, average = 'weighted'), '\\n')\n",
    "    print('Initial Dev Accuracy:', model.score(nX_dev, y_dev))\n",
    "    print('Initial Dev F1 score:', metrics.f1_score(y_dev, pred_y_dev, average = 'weighted'), '\\n')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_dev, pred_y_dev)\n",
    "    print('Confusion Matrix for Dev: \\n\\n', cm, \"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "naive_bayes_bern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Bernoulli and Gaussian models\n",
    "\n",
    "Let's look at creating two separate models -- one Bernoulli and one Gaussian. Then, we will use the probabilites from those models as input to a final Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4429</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>215.564376</td>\n",
       "      <td>192</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>205.548048</td>\n",
       "      <td>201</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>384.480169</td>\n",
       "      <td>324</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>313.180459</td>\n",
       "      <td>309</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10572</th>\n",
       "      <td>296.325834</td>\n",
       "      <td>295</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11892</th>\n",
       "      <td>212.084889</td>\n",
       "      <td>212</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total_Distance_To_Hydrology  Horizontal_Distance_To_Hydrology  \\\n",
       "Id                                                                     \n",
       "4429                      0.000000                                 0   \n",
       "12400                     0.000000                                 0   \n",
       "4648                    215.564376                               192   \n",
       "5954                      0.000000                                 0   \n",
       "2947                      0.000000                                 0   \n",
       "9253                    205.548048                               201   \n",
       "3472                    384.480169                               324   \n",
       "3079                    313.180459                               309   \n",
       "10572                   296.325834                               295   \n",
       "11892                   212.084889                               212   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  \n",
       "Id                                     \n",
       "4429                                0  \n",
       "12400                               0  \n",
       "4648                               98  \n",
       "5954                                0  \n",
       "2947                                0  \n",
       "9253                               43  \n",
       "3472                              207  \n",
       "3079                               51  \n",
       "10572                              28  \n",
       "11892                               6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new dataframes with original columns + Total_Distance_To_Hydrology\n",
    "train2 = train_data.copy()\n",
    "dev2 = dev_data.copy()\n",
    "\n",
    "# Total_Distance_to_Hydrology\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "train2['Total_Distance_To_Hydrology'] = np.sqrt(train_data[\"Horizontal_Distance_To_Hydrology\"]**2 + train_data['Vertical_Distance_To_Hydrology']**2)\n",
    "dev2['Total_Distance_To_Hydrology'] = np.sqrt(dev_data[\"Horizontal_Distance_To_Hydrology\"]**2 + dev_data['Vertical_Distance_To_Hydrology']**2)\n",
    "train2[[\"Total_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into two different dataframes:\n",
    "\n",
    "# Continuous Data Columns\n",
    "continuous_cols =  [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Roadways\",\n",
    "                   \"Horizontal_Distance_To_Fire_Points\", \"Total_Distance_To_Hydrology\",\n",
    "                   \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "\n",
    "# Binary / Categorical Data Columns\n",
    "categorical_cols = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "                   'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "                   'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "                   'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "                   'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "                   'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "                   'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "                   'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "                   'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "                   'Soil_Type39', 'Soil_Type40', 'Wilderness_Area1', 'Wilderness_Area2',\n",
    "                   'Wilderness_Area3', 'Wilderness_Area4']\n",
    "\n",
    "# Defining new train and dev data sets split by above lists\n",
    "# Continuous dataframes\n",
    "Xtrain_G = train2[continuous_cols]\n",
    "Xdev_G = dev2[continuous_cols]\n",
    "# Categorical dataframes\n",
    "Xtrain_C = train2[categorical_cols]\n",
    "Xdev_C = dev2[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Soil_Type1</th>\n",
       "      <th>Soil_Type2</th>\n",
       "      <th>Soil_Type3</th>\n",
       "      <th>Soil_Type4</th>\n",
       "      <th>Soil_Type5</th>\n",
       "      <th>Soil_Type6</th>\n",
       "      <th>Soil_Type7</th>\n",
       "      <th>Soil_Type8</th>\n",
       "      <th>Soil_Type9</th>\n",
       "      <th>Soil_Type10</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <th>Wilderness_Area4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Soil_Type1  Soil_Type2  Soil_Type3  Soil_Type4  Soil_Type5  Soil_Type6  \\\n",
       "Id                                                                              \n",
       "5415            0           0           0           0           0           0   \n",
       "6658            0           0           0           0           0           0   \n",
       "3551            0           1           0           0           0           0   \n",
       "5155            0           0           0           0           0           0   \n",
       "10748           0           0           1           0           0           0   \n",
       "\n",
       "       Soil_Type7  Soil_Type8  Soil_Type9  Soil_Type10  ...  Soil_Type35  \\\n",
       "Id                                                      ...                \n",
       "5415            0           0           0            1  ...            0   \n",
       "6658            0           0           0            0  ...            0   \n",
       "3551            0           0           0            0  ...            0   \n",
       "5155            0           0           0            0  ...            0   \n",
       "10748           0           0           0            0  ...            0   \n",
       "\n",
       "       Soil_Type36  Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  \\\n",
       "Id                                                                       \n",
       "5415             0            0            0            0            0   \n",
       "6658             0            0            0            1            0   \n",
       "3551             0            0            0            0            0   \n",
       "5155             0            0            0            0            0   \n",
       "10748            0            0            0            0            0   \n",
       "\n",
       "       Wilderness_Area1  Wilderness_Area2  Wilderness_Area3  Wilderness_Area4  \n",
       "Id                                                                             \n",
       "5415                  0                 0                 0                 1  \n",
       "6658                  0                 0                 1                 0  \n",
       "3551                  0                 0                 0                 1  \n",
       "5155                  0                 0                 0                 1  \n",
       "10748                 0                 0                 1                 0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "Xdev_C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Model Dev Accuracy: 0.6025\n",
      "Bernoulli Model Dev Accuracy: 0.5827 \n",
      "\n",
      "Combined Model Train Accuracy: 0.3357\n",
      "Combined Model Dev Accuracy: 0.3528\n"
     ]
    }
   ],
   "source": [
    "def two_models_combined():\n",
    "    \n",
    "    # Make copies of the categorical dataframes and rename\n",
    "    cX_train = Xtrain_C.copy()\n",
    "    cX_dev = Xdev_C.copy()\n",
    "\n",
    "    # Normalize continuous variables and create continuous dataframes\n",
    "    scaler = StandardScaler()\n",
    "    nX_train = scaler.fit_transform(Xtrain_G)\n",
    "    nX_dev = scaler.transform(Xdev_G)\n",
    "    \n",
    "    # Define labels\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    # Create a Categorical Model and a Gaussian Model\n",
    "    model_G = GaussianNB()\n",
    "    model_G.fit(nX_train, y_train)\n",
    "    print(\"Gaussian Model Dev Accuracy: %3.4f\" %model_G.score(nX_dev, y_dev))\n",
    "    model_C = BernoulliNB()\n",
    "    model_C.fit(cX_train, y_train)\n",
    "    print(\"Bernoulli Model Dev Accuracy: %3.4f\" %model_C.score(cX_dev, y_dev), '\\n')\n",
    "\n",
    "    # Get probability predictions for each model\n",
    "    # On training data\n",
    "    G_train_probs = model_G.predict_proba(nX_train)\n",
    "    C_train_probs = model_C.predict_proba(cX_train)\n",
    "\n",
    "    # On dev data\n",
    "    G_dev_probs = model_G.predict_proba(nX_dev)\n",
    "    C_dev_probs = model_C.predict_proba(cX_dev)\n",
    "\n",
    "    # Combine probability prediction for class=1 from both models into a 2D array\n",
    "    # np.c_ translates slice objects to concatenation along the second axis.\n",
    "    X_new_train = np.c_[(G_train_probs[:,1], C_train_probs[:,1])] # Train\n",
    "    X_new_dev = np.c_[(G_dev_probs[:,1], C_dev_probs[:,1])] # Dev\n",
    "\n",
    "    # Fit final Gaussian model\n",
    "    # Using the probabilities from the last two modes as input\n",
    "    model = GaussianNB()\n",
    "    clf = model.fit(X_new_train, y_train)\n",
    "\n",
    "    # Predict class labels on dev data\n",
    "    pred_labels_train = model.predict(X_new_train)\n",
    "    pred_labels_dev = model.predict(X_new_dev)\n",
    "\n",
    "    # Print results\n",
    "    train_score = model.score(X_new_train, y_train)\n",
    "    dev_score = model.score(X_new_dev, y_dev)\n",
    "    print('Combined Model Train Accuracy: %3.4f' %train_score)\n",
    "    print('Combined Model Dev Accuracy: %3.4f' %dev_score)\n",
    "    \n",
    "two_models_combined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that worked better than simply throwing all the data into one model (as was done in the beginning), it still isn't better than when we removed the binary data columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Multinomial Model (MultinomialNB)\n",
    "\n",
    "Now, I'll take the continuous features and transform them into multinomial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrain_G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-75879a6701a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mnum_divisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_thresholds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_divisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Thresholds for Train Data:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultifeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtrain_G' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating thresholds based on quantiles\n",
    "\n",
    "def define_thresholds(data, num_divisions):\n",
    "    thresholds = np.zeros([len(data.columns), num_divisions-1])\n",
    "    step = round(1/num_divisions,2)\n",
    "    prev = 0\n",
    "    bins = []\n",
    "    for i in range(num_divisions-1):\n",
    "        num = prev\n",
    "        bins.append(num+step)\n",
    "        prev = num+step\n",
    "    print(\"Quantile Cutoffs:\", bins)\n",
    "    \n",
    "    i = 0\n",
    "    for item in Xtrain_G.columns:\n",
    "        for j in range(len(bins)):\n",
    "            thresholds[i][j] = int(data[item].quantile(bins[j]))\n",
    "        i+=1\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "# threshold = NxM array, N = number of columns, M = number of bins\n",
    "# inputs: dataframe, threshold values\n",
    "# outputs: new dataframe\n",
    "def multifeature(data, thresholds):\n",
    "    # capture column names\n",
    "    features = list(data.columns)\n",
    "    \n",
    "    # initiate a new dataframe \n",
    "    new_df = data.copy()\n",
    "\n",
    "    i=0\n",
    "    # bin the data\n",
    "    for feature in features:\n",
    "        new_df[feature] = np.digitize(np.array(data[feature]), thresholds[i])\n",
    "        i+=1\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "num_divisions = 4\n",
    "thresholds = define_thresholds(Xtrain_G, num_divisions)\n",
    "print(\"Thresholds for Train Data:\\n\", thresholds, '\\n')\n",
    "train_df = multifeature(Xtrain_G, thresholds)\n",
    "dev_df = multifeature(Xdev_G, thresholds)\n",
    "\n",
    "#thresholds = define_thresholds(Xdev_G, num_divisions)\n",
    "#print(\"Thresholds for Dev Data:\\n\", thresholds)\n",
    "\n",
    "# This will merge our newly created multinomial features with the original binary features\n",
    "Xtrain_multi = pd.merge(train_df, Xtrain_C, left_on='Id', right_on='Id', how='left')\n",
    "Xtrain_multi.shape\n",
    "\n",
    "Xdev_multi = pd.merge(dev_df, Xdev_C, left_on='Id', right_on='Id', how='left')\n",
    "Xdev_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Train Accuracy: 0.6271494708994709\n",
      "Initial Train F1 score: 0.626000443375661 \n",
      "\n",
      "Initial Dev Accuracy: 0.6253306878306878\n",
      "Initial Dev F1 score: 0.6257925845137267 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[228 106   3   0  48   1  50]\n",
      " [ 89 245   7   0  77  13   3]\n",
      " [  0   2 194  53  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 67  57  25   0 262  14   0]\n",
      " [  5  11  99  27  42 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.6271494708994709\n",
      "Initial Train F1 score: 0.626000443375661 \n",
      "\n",
      "Initial Dev Accuracy: 0.6253306878306878\n",
      "Initial Dev F1 score: 0.6257925845137267 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[228 106   3   0  48   1  50]\n",
      " [ 89 245   7   0  77  13   3]\n",
      " [  0   2 194  53  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 67  57  25   0 262  14   0]\n",
      " [  5  11  99  27  42 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.6271494708994709\n",
      "Initial Train F1 score: 0.626000443375661 \n",
      "\n",
      "Initial Dev Accuracy: 0.6253306878306878\n",
      "Initial Dev F1 score: 0.6257925845137267 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[228 106   3   0  48   1  50]\n",
      " [ 89 245   7   0  77  13   3]\n",
      " [  0   2 194  53  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 67  57  25   0 262  14   0]\n",
      " [  5  11  99  27  42 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.6271494708994709\n",
      "Initial Train F1 score: 0.626000443375661 \n",
      "\n",
      "Initial Dev Accuracy: 0.6253306878306878\n",
      "Initial Dev F1 score: 0.6257925845137267 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[228 106   3   0  48   1  50]\n",
      " [ 89 245   7   0  77  13   3]\n",
      " [  0   2 194  53  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 67  57  25   0 262  14   0]\n",
      " [  5  11  99  27  42 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.6272321428571429\n",
      "Initial Train F1 score: 0.6260625077551876 \n",
      "\n",
      "Initial Dev Accuracy: 0.6256613756613757\n",
      "Initial Dev F1 score: 0.626116469661418 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[228 106   3   0  48   1  50]\n",
      " [ 89 246   7   0  76  13   3]\n",
      " [  0   2 194  53  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 67  57  25   0 262  14   0]\n",
      " [  5  11  99  27  42 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n",
      "\n",
      "Initial Train Accuracy: 0.6273148148148148\n",
      "Initial Train F1 score: 0.6260733387361846 \n",
      "\n",
      "Initial Dev Accuracy: 0.6246693121693122\n",
      "Initial Dev F1 score: 0.6250483099846758 \n",
      "\n",
      "Confusion Matrix for Dev: \n",
      "\n",
      " [[227 106   3   0  49   1  50]\n",
      " [ 89 245   7   0  76  14   3]\n",
      " [  0   2 193  54  38 136   0]\n",
      " [  0   0  51 380   0  24   0]\n",
      " [ 69  54  25   0 262  15   0]\n",
      " [  4  11  99  27  43 244   0]\n",
      " [ 39  35   0   0  11   0 338]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - using MultinomialNB\n",
    "# -----------------------------------------------------------------------------\n",
    "def naive_bayes_multi():\n",
    "\n",
    "    # Create new dataframes with normalized values of data\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    # Generating initial model\n",
    "    smoothing_list = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    for num in smoothing_list:\n",
    "\n",
    "        model = MultinomialNB(alpha=num)\n",
    "        model.fit(Xtrain_multi, y_train)\n",
    "        pred_y_train = model.predict(Xtrain_multi)\n",
    "        pred_y_dev = model.predict(Xdev_multi)\n",
    "\n",
    "        # Print the results\n",
    "        print('\\nInitial Train Accuracy:', model.score(Xtrain_multi, y_train))\n",
    "        print('Initial Train F1 score:', metrics.f1_score(y_train, pred_y_train, average = 'weighted'), '\\n')\n",
    "        print('Initial Dev Accuracy:', model.score(Xdev_multi, y_dev))\n",
    "        print('Initial Dev F1 score:', metrics.f1_score(y_dev, pred_y_dev, average = 'weighted'), '\\n')\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_dev, pred_y_dev)\n",
    "        print('Confusion Matrix for Dev: \\n\\n', cm, \"\\n\")\n",
    "\n",
    "naive_bayes_multi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Copy data to new variable\n",
    "train3 = train_data.copy()\n",
    "dev3 = dev_data.copy()\n",
    "\n",
    "# Total_Distance_to_Hydrology\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "train3['Total_Distance_To_Hydrology'] = np.sqrt(train_data[\"Horizontal_Distance_To_Hydrology\"]**2 + train_data['Vertical_Distance_To_Hydrology']**2)\n",
    "dev3['Total_Distance_To_Hydrology'] = np.sqrt(dev_data[\"Horizontal_Distance_To_Hydrology\"]**2 + dev_data['Vertical_Distance_To_Hydrology']**2)\n",
    "\n",
    "\n",
    "# Plotting features again\n",
    "# Drop features (ONLY RUN ONCE)\n",
    "drop = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "       'Soil_Type39', 'Soil_Type40', \"Horizontal_Distance_To_Hydrology\",\n",
    "        \"Vertical_Distance_To_Hydrology\", 'Wilderness_Area1', 'Wilderness_Area2',\n",
    "       'Wilderness_Area3', 'Wilderness_Area4']\n",
    "\n",
    "train4 = train3.drop(drop, axis = 1)\n",
    "dev4 = dev3.drop(drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12096, 8)\n",
      "\n",
      "Initial Train Accuracy: 0.6025132275132276\n",
      "Initial Train F1 score: 0.5911853094372181 \n",
      "\n",
      "Initial Dev Accuracy: 0.6025132275132276\n",
      "Initial Dev F1 score: 0.5909465797445023 \n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      " [[239  54   0   0  37   4 102]\n",
      " [119 164  13   1  95  17  25]\n",
      " [  0   6 164  93  54 106   0]\n",
      " [  0   0  28 378   0  49   0]\n",
      " [  3  80   6   0 305  31   0]\n",
      " [  0  14 101  65  43 205   0]\n",
      " [ 52   1   0   0   3   0 367]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PCA Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_NB():\n",
    "    # Normalize data\n",
    "    # Using StandardScalar() to normalize data\n",
    "    scaler = StandardScaler()\n",
    "    nX_train = scaler.fit_transform(train4)\n",
    "    nX_dev = scaler.transform(dev4)\n",
    "    \n",
    "    # Define labels\n",
    "    y_train = train_labels.Cover_Type\n",
    "    y_dev = dev_labels.Cover_Type\n",
    "\n",
    "    pca = PCA(n_components='mle', svd_solver='full')\n",
    "\n",
    "    # Create 2D projected data\n",
    "    data2D = pca.fit_transform(nX_train)\n",
    "    dev2D = pca.transform(nX_dev)\n",
    "    print(data2D.shape)\n",
    "\n",
    "    model = GaussianNB(var_smoothing=0.00001)\n",
    "    model.fit(nX_train, y_train)\n",
    "    pred_y_train = model.predict(nX_train)\n",
    "    pred_y_dev = model.predict(nX_dev)\n",
    "\n",
    "    # Print the results\n",
    "    print('\\nInitial Train Accuracy:', model.score(nX_train, y_train))\n",
    "    print('Initial Train F1 score:', metrics.f1_score(y_train, pred_y_train, average = 'weighted'), '\\n')\n",
    "    print('Initial Dev Accuracy:', model.score(nX_dev, y_dev))\n",
    "    print('Initial Dev F1 score:', metrics.f1_score(y_dev, pred_y_dev, average = 'weighted'), '\\n')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(dev_labels, pred_y_dev)\n",
    "    print('Confusion Matrix: \\n\\n', cm, \"\\n\")\n",
    "\n",
    "pca_NB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
