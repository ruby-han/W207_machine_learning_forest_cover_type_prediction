{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqnkWOMaA00A"
   },
   "source": [
    "# Forest Cover Type Prediction - Final Report\n",
    "The goal of this project is to build a model that uses cartographic features about a cell of forest land to accurately predict the predominant kind of tree cover for the cell.\n",
    "\n",
    "## The Inference Problem\n",
    "**X:** Cartographic fatures of forest land, such as elevation, slope, distance to water, shade, and soil type.\n",
    "\n",
    "**y:** Predominant class of tree cover for the given cell.\n",
    "\n",
    "**Model:** We will train a variety of models, including KNN, Naive Bayes, Decision Trees, and Others.\n",
    "\n",
    "**Parameters:** Each of the different models will encode the training data via the parameters. KNN is the exception, and no parameters will be stored.\n",
    "\n",
    "**Cost Functions:** Each model will employ a different cost function. For example, decision trees will use entropy.\n",
    "\n",
    "**Objective:** Each of the models will have their own objective, like maximizing likelihood, in the case of Naivve bayes.\n",
    "\n",
    "## [Data Source](https://www.kaggle.com/c/forest-cover-type-prediction)\n",
    "The outcome variable (forest cover type) comes from the US Forest Service, while the feature variables come from a combination of the US Geological survey as well as the USFS. This data encapsulates four wilderness areas in the Roosevelt National Forest; because these areas are preserved from most human disturbance, we assume forest cover types are a result of natural processes represented by the independent variables (although this assumption is not necessary to generate an effective model).\n",
    "\n",
    "## Feature Definitions\n",
    "The raw data contains a mixture of continuous and binary variables, as defined below: \n",
    "\n",
    "- `Elevation` - Elevation in meters\n",
    "- `Aspect` - Aspect in degrees azimuth\n",
    "- `Slope` - Slope in degrees\n",
    "- `Horizontal_Distance_To_Hydrology` - Horz Dist to nearest surface water features\n",
    "- `Vertical_Distance_To_Hydrology` - Vert Dist to nearest surface water features\n",
    "- `Horizontal_Distance_To_Roadways` - Horz Dist to nearest roadway\n",
    "- `Hillshade_9am` (0 to 255 index) - Hillshade index at 9am, summer solstice\n",
    "- `Hillshade_Noon` (0 to 255 index) - Hillshade index at noon, summer solstice\n",
    "- `Hillshade_3pm` (0 to 255 index) - Hillshade index at 3pm, summer solstice\n",
    "- `Horizontal_Distance_To_Fire_Points` - Horz Dist to nearest wildfire ignition points\n",
    "- `Wilderness_Area` (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n",
    "- `Soil_Type` (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n",
    "- `Cover_Type` (7 types, integers 1 to 7) - Forest Cover Type designation\n",
    "\n",
    "Additional transformed variables to potentially train our models:\n",
    "- `Wilderness_Area` - combining the 4 binary columns into one categorical variable (1-4), making an assumption about exclusivity of areas that needs to be checked\n",
    "- `Soil_Type` - combining the 40 binary columns into one categorical variable (1-40), making an assumption about exclusivity of soil types that needs to be checked\n",
    "- `Total_Distance_To_Hydrology` - Euclidean distance using \"Horizontal\" and \"Vertical\" distances\n",
    "- Binned versions of continuous variables\n",
    "\n",
    "## Testing Plan\n",
    "We plan to tune and compare a variety of models, optimizing toward the highest possible $F_1$ score (a metric which balances precision and recall). All models will be trained on labeled data, and tested against \"development\" data using a 50-50 split. \n",
    "\n",
    "Potential models to test include:\n",
    "- k Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "\n",
    "## <a id = 0> </a>Navigation\n",
    "- [Data Load](#1)\n",
    "- [Data Split](#2)\n",
    "- [Exploratory Data Analyses](#3)\n",
    "    - [Histogram](#4)\n",
    "    - [Scatter Plots](#5)\n",
    "    - [Correlation Matrix](#6)\n",
    "    - [Box Plots](#7)\n",
    "    - [Violin Plots](#8)\n",
    "    - [Wilderness Area and Soil Types](#9)\n",
    "- [Confusion Matrix](#9.5)\n",
    "- [Data Transformation](#9.6)\n",
    "- [Model Building](#10)\n",
    "- [Result Analyses](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgCVuPjJ_DB_"
   },
   "source": [
    "## <a id = 1> </a> Data Load\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CVzHgUNb_DCC"
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn - learning libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# SK-learn - feature processing libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# SK-learn - evaluation libraries\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Producing Decision Tree diagrams\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "from subprocess import call\n",
    "\n",
    "# Other\n",
    "import copy\n",
    "from textwrap import wrap\n",
    "\n",
    "# Expand rows/columns in df outputs\n",
    "pd.set_option(\n",
    "#     'max_rows', None, \n",
    "    'max_columns', None,\n",
    "    'max_colwidth', None\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 2> </a> Data Split\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/processed/train_data.csv').set_index('Id')\n",
    "train_labels = pd.read_csv('../data/processed/train_labels.csv').set_index('Id')\n",
    "dev_data = pd.read_csv('../data/processed/dev_data.csv').set_index('Id')\n",
    "dev_labels = pd.read_csv('../data/processed/dev_labels.csv').set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (12096, 54)\n",
      "Train Labels Shape: (12096, 1)\n",
      "Dev Data Shape: (3024, 54)\n",
      "Dev Labels Shape: (3024, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Train Data Shape: {train_data.shape}'\n",
    "    f'\\nTrain Labels Shape: {train_labels.shape}'\n",
    "    f'\\nDev Data Shape: {dev_data.shape}'\n",
    "    f'\\nDev Labels Shape: {dev_labels.shape}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwr7mLW_LOoq"
   },
   "source": [
    "## <a id = 3> </a>Exploratory Data Analyses\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_agWMq__DCE",
    "outputId": "a314c7e5-3e96-48db-d6f6-9e6807969937"
   },
   "outputs": [],
   "source": [
    "# Column List\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "OvljDYI_7cdU",
    "outputId": "dc55a1b9-ce82-4864-81e6-3c84fd0dceee"
   },
   "outputs": [],
   "source": [
    "# Statistics Summary\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types for each field\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- All data fields are int64 objects\n",
    "- `Wilderness_Area` and `Soil_Type` are binary features\n",
    "- `Cover_Type` is categorized from 1-7\n",
    "- The rest of the fields are continuous\n",
    "- No null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bcQ8miO9RdS"
   },
   "source": [
    "### <a id = 4> </a>Histograms of each non-binary feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Given Random split between train and dev, we would expect the training distributions to compare similarly to our dev data. This will be key in in generalization both across Dev data, as well as final test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "d1yYGw5M9qDF",
    "outputId": "e409242a-a078-4d7c-856d-6f29edf11fd1"
   },
   "outputs": [],
   "source": [
    "# Note: When you export this notebook and run it in Jupyter Lab, you need to\n",
    "# reset the first column of test_kaggle as you did with train and test:\n",
    "# test_data = test_kaggle.set_index('Id')\n",
    "# Otherwise, the last row of graphs are one off\n",
    "\n",
    "# Strip underscores from feature names for nice printing\n",
    "formatted_cols = copy.deepcopy(X_train_df.columns).str.replace('_', ' ')\n",
    "\n",
    "# Plot Formatting\n",
    "plt.rcParams.update({'text.color' : \"dimgrey\",\n",
    "                     'axes.labelcolor' : \"grey\"})\n",
    "\n",
    "# include dev_data in plots for comparison\n",
    "# datasets = [train_data, dev_data]    \n",
    "# data_names = ['train', 'dev']\n",
    "\n",
    "datasets = [X_train_df]\n",
    "data_names = ['train']\n",
    "\n",
    "# For Train, Dev, and Test, plot each non-binary feature\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through to show hist of non-binary for each \n",
    "for d, data in enumerate(datasets):    # For each dataset (only needed when comparing dev and train)\n",
    "    for i in np.arange(0, 10):          # For each non-binary figure in dataset\n",
    "\n",
    "        \n",
    "        \n",
    "        data.iloc[:, i].plot.hist(ax = axes[i], \n",
    "                                    figsize = (20,5), \n",
    "                                    sharex = True, color = '#1c4966')\n",
    "\n",
    "        \n",
    "        # Column and Row names for each plot\n",
    "        if (i == 0) and (d == 0):    # Top Left Corner\n",
    "            axes[i].set_ylabel(data_names[d])\n",
    "            axes[i].set_title(\"\\n\".join(wrap(formatted_cols[i], 12)))\n",
    "        \n",
    "        elif i == 0:    # First Column\n",
    "            axes[i].set_ylabel(data_names[d])\n",
    "    \n",
    "        elif d == 0:    # First Row\n",
    "            axes[i].set_ylabel('')\n",
    "            axes[i].set_title(\"\\n\".join(wrap(formatted_cols[i], 12)))\n",
    "        else:\n",
    "            axes[i].set_ylabel('')\n",
    "            \n",
    "        # For All Plots\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_color('grey')\n",
    "        axes[i].spines['bottom'].set_color('grey')\n",
    "        axes[i].tick_params(colors = 'grey')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbAfF1cMn57"
   },
   "source": [
    "### <a id = 5> </a> Scatterplots comparing each feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Scatterplots may reveal correlational relationships between features. Additionally, the color of each datapoint represents a forest cover type. This will also help reveal if the relationship between two features varies by forest cover type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "rUD1ZQl8M2A5",
    "outputId": "7787543d-932f-49a0-a9af-006d5edbe618"
   },
   "outputs": [],
   "source": [
    "# Scatterplot Matrix\n",
    "# ------------------------------------------------------------------------------\n",
    "# This isn't meant to be a final output (obviously it's too much in its current\n",
    "# state); just wanted to see all of the distributions at once so we could pick\n",
    "# out meaningful ones \n",
    "# Currently, this takes a long time to run.\n",
    "\n",
    "train_data_copy = X_train_df.copy().iloc[:, :10]\n",
    "train_data_copy[\"Cover_Type\"] = X_train_df.Cover_Type\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "Flgtn5cXCGYw",
    "outputId": "94297b80-7f9e-4ea7-e306-c1acaf0e6444"
   },
   "outputs": [],
   "source": [
    "# Cutting down the number of columns\n",
    "columns = [\"Elevation\", \"Aspect\", \"Slope\", \"Hillshade_9am\",\n",
    "           \"Hillshade_Noon\", \"Hillshade_3pm\", \"Cover_Type\"]\n",
    "\n",
    "train_data_copy2 = X_train_df.copy().loc[:, columns]\n",
    "train_data_copy2\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy2, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "Hs3lJQDLCG9p",
    "outputId": "2db785fa-1413-492c-cc05-9e1bd2feaa63"
   },
   "outputs": [],
   "source": [
    "# Rest of the columns\n",
    "columns = [\"Horizontal_Distance_To_Hydrology\",\n",
    "           \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "           \"Horizontal_Distance_To_Fire_Points\", \"Hillshade_9am\",\n",
    "           \"Hillshade_Noon\", \"Hillshade_3pm\", \"Cover_Type\"]\n",
    "\n",
    "train_data_copy3 = X_train_df.copy().loc[:, columns]\n",
    "train_data_copy3\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy3, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOOnDjwXMzAa"
   },
   "source": [
    "### <a id = 6> </a>Correlation Matrix - Relationships between each non-binary feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Comparing the train_data heatmap to dev_data, it is evident that they have largely the same correlation structure. This is expected given the random 80/20 split, but it is important to note any deviations in structure will lead to poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "b6ZXlw7cNYJI",
    "outputId": "bde80405-f95d-477b-d2ed-c930758e6c6d"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey = True, figsize = (20,10))\n",
    "\n",
    "datasets = [X_train_df, X_dev_df]    \n",
    "data_names = ['train', 'dev']\n",
    "\n",
    "# Correlation plot for each dataset - numeric values\n",
    "for i, data in enumerate(datasets):    # For each dataset\n",
    "\n",
    "    corr = data.iloc[:, :10].corr()    # Set the correlation matrix\n",
    "    \n",
    "    # Mask to upper triangular\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    sns.heatmap(corr, \n",
    "                xticklabels = corr.columns.values,\n",
    "                yticklabels = corr.columns.values, \n",
    "                cmap = 'bwr', \n",
    "                annot = True, \n",
    "                mask = mask, \n",
    "                fmt = '.2f',\n",
    "                ax = axes[i],\n",
    "                cbar = False).set(title = data_names[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 7> </a>Boxplots for Numeric Features\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Cartographic features like Elevation, Aspect, and Slope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 1, figsize = (20, 35))\n",
    "\n",
    "feature_cols = [\"Elevation\", \"Aspect\", \"Slope\", \n",
    "                \"Horizontal_Distance_To_Roadways\", \n",
    "                \"Horizontal_Distance_To_Fire_Points\", \"Hillshade_9am\",\n",
    "                \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "                \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\"] # Added these to feature_cols\n",
    "\n",
    "for i, var in enumerate(feature_cols):\n",
    "    sns.boxplot(x = var, data = X_train_df, ax = ax[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 8> </a>Violin Plot on Continuous Features\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot\n",
    "fig, axes = plt.subplots(5, 2, figsize = (20, 20))\n",
    "col_list = ['Elevation', 'Aspect', 'Slope',\n",
    "            'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am',\n",
    "            'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "i = 0\n",
    "for col_name in col_list:\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    sns.violinplot(x='Cover_Type', y=col_name, data=X_train_df , ax=axes[row][col])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- `Elevation` varies according to `Cover_Type` indicating that this will be an important variable for prediction\n",
    "- `Horizontal_Distance_To_Hydrology` and `Horizontal_Distance_To_Roadways` have similar distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 9> </a>`Wilderness_Area` and `Soil_Type` Binary Features Exploration\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Unpivot `Wilderness_Area` and `Soil_Type` Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_list = []\n",
    "for i in range(40):\n",
    "    soil_list.append(f'Soil_Type{i+1}')\n",
    "\n",
    "wild_area_list = ['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot df from wide to long format by combining `Soil_Type#` and `Wilderness_Area#` to one column each\n",
    "X_train_df_comb = pd.melt(X_train_df, \n",
    "                          id_vars=col_list+soil_list+['Cover_Type'], \n",
    "                          value_vars=wild_area_list, \n",
    "                          var_name='Wilderness_Area')\n",
    "X_train_df_comb2 = X_train_df_comb[X_train_df_comb.value != 0].drop(columns=['value'])\n",
    "\n",
    "X_train_df_comb3 = pd.melt(X_train_df_comb2, \n",
    "                          id_vars=col_list+['Wilderness_Area', 'Cover_Type'], \n",
    "                          value_vars=soil_list, \n",
    "                          var_name='Soil_Type')\n",
    "X_train_df_comb4 = X_train_df_comb3[X_train_df_comb3.value != 0].drop(columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot - Combined Wilderness Area\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.countplot(x='Wilderness_Area', hue='Cover_Type', data=X_train_df_comb4)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x()-0.001, p.get_height()+10))\n",
    "plt.legend(loc='upper right', title='Cover Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Cover Type 4 only exists in Wilderness Area 4\n",
    "- Fairly equal representation of wilderness areas, except for Wilderness Area 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot - Soil Type\n",
    "plt.figure(figsize=(50, 10))\n",
    "ax = sns.countplot(x='Soil_Type', hue='Cover_Type', data=X_train_df_comb4)\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height():.0f}', (p.get_x(), p.get_height()+10))\n",
    "plt.legend(loc='upper right', title='Cover Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- There are no cover types for Soil Type 7 and 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys_r_uq2NpWm"
   },
   "source": [
    "### Explore Wilderness Area Binary Counts\n",
    "Fairly equal representation of wilderness areas, except for Wilderness Area 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLX-cerx_DCF",
    "outputId": "e0e331da-20b3-4ab2-e53d-f8cac5ac1cdd"
   },
   "outputs": [],
   "source": [
    "X_train_df.groupby(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'])['Cover_Type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzgc_iuUN31T"
   },
   "source": [
    "### Determining if one Soil type exists for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieqRiz3g7lEe",
    "outputId": "ca72ed18-9758-4ebc-814b-63c45bc7e1d8"
   },
   "outputs": [],
   "source": [
    "# Determining if one soil type exists for each data point\n",
    "soil_type_cols = [col_name for col_name in X_train_df.columns if 'Soil_Type' in col_name]\n",
    "\n",
    "X_train_df_soil = X_train_df.copy()\n",
    "X_train_df_soil['Soil_Type_Count'] = X_train_df_soil[soil_type_cols].sum(axis = 1)\n",
    "X_train_df_soil['Soil_Type_Count'].value_counts()\n",
    "\n",
    "# Only 1 soil type exists for each row - no mix of different soil types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaNm4CEDUNSD",
    "outputId": "a2ce4b33-bc59-4dbd-b8af-78851dbe963a"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     dev_data.drop(columns = 'Soil_Type_Count', axis = 1)\n",
    "# except:\n",
    "#     print('Soil_Type_Count not yet created')\n",
    "\n",
    "X_dev_df_soil = X_dev_df.copy()\n",
    "X_dev_df_soil['Soil_Type_Count'] = X_dev_df_soil[soil_type_cols].sum(axis = 1)\n",
    "X_dev_df_soil['Soil_Type_Count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-5xnoMY8O_y"
   },
   "source": [
    "\n",
    "Looks like both Soil_Types and Wilderness_Areas are mutually exclusive within the columns (only 1 area/type per row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV5Zpmf8am45"
   },
   "outputs": [],
   "source": [
    "# Wilderness Areas and Soil Types\n",
    "# ------------------------------------------------------------------------------\n",
    "# Combining wilderness areas into one column, soil types into one column\n",
    "\n",
    "def get_feature_number(r, col_prefix):\n",
    "  \n",
    "  # gets the column name suffix of the true variable (wilderness area/soil type)\n",
    "    cols = [col_name for col_name in r.index if (col_prefix in col_name) and re.search(r'\\d', col_name) is not None]\n",
    "\n",
    "\n",
    "    feature_subix = r[cols].argmax()\n",
    "\n",
    "  #\n",
    "    feature_name = cols[feature_subix]\n",
    "    feature_num = ''.join([i for i in feature_name if i.isdigit()])\n",
    "    return int(feature_num)\n",
    "\n",
    "X_train_df_DR = X_train_df_soil.copy()\n",
    "X_dev_df_DR = X_dev_df_soil.copy()\n",
    "\n",
    "X_train_df_DR['Wilderness_Area'] = X_train_df_DR.apply(lambda x:get_feature_number(x, 'Wilderness_Area'), axis = 1)\n",
    "X_train_df_DR['Soil_Type'] = X_train_df_DR.apply(lambda x:get_feature_number(x, 'Soil_Type'), axis = 1)\n",
    "X_dev_df_DR['Wilderness_Area'] = X_dev_df_DR.apply(lambda x:get_feature_number(x, 'Wilderness_Area'), axis = 1)\n",
    "X_dev_df_DR['Soil_Type'] = X_dev_df_DR.apply(lambda x:get_feature_number(x, 'Soil_Type'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total_Distance_to_Hydrology\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "X_train_df_DR['Total_Distance_To_Hydrology'] = np.sqrt(X_train_df_DR[\"Horizontal_Distance_To_Hydrology\"]**2 + X_train_df_DR['Vertical_Distance_To_Hydrology']**2)\n",
    "X_dev_df_DR['Total_Distance_To_Hydrology'] = np.sqrt(X_dev_df_DR[\"Horizontal_Distance_To_Hydrology\"]**2 + X_dev_df_DR['Vertical_Distance_To_Hydrology']**2)\n",
    "X_train_df_DR[[\"Total_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_7oD2jp3dV3",
    "outputId": "e292ed4c-db2d-4366-d2d7-2785d4ad42da"
   },
   "outputs": [],
   "source": [
    "X_train_df_DR['Wilderness_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIdJQPP08UNE",
    "outputId": "ff71aa54-4209-4a3f-df39-89685aee2a6e"
   },
   "outputs": [],
   "source": [
    "X_train_df_DR['Soil_Type'].value_counts().sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 9.5> </a>Confusion Matrix \n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_NB_confusion_matrix():\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(train_data.iloc[:, :10])\n",
    "    X_dev_std = scaler.transform(dev_data.iloc[:, :10])\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train_std, train_labels.values.ravel())\n",
    "    dev_pred = model.predict(X_dev_std)\n",
    "    \n",
    "    nb_f1_score = metrics.f1_score(dev_pred, dev_labels, average = 'weighted')\n",
    "    \n",
    "    print(f'Gaussian NB f1_score: {nb_f1_score:.4f}\\n')\n",
    "    \n",
    "    # Print confusion matrix in ASCII form\n",
    "    conf_matrix = confusion_matrix(dev_labels, dev_pred)\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Produce confusion matrix in the form of heatmap\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "    cmx = ax.matshow(conf_matrix, cmap=plt.cm.Accent)\n",
    "    plt.colorbar(cmx)\n",
    "    \n",
    "    plt.title('Confusion Matrix Heat Map')\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Accent)\n",
    "    classNames = [str(i+1) for i in range(conf_matrix.shape[0])]    \n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=0, fontsize=14)\n",
    "    plt.yticks(tick_marks, classNames, fontsize=14)\n",
    "   \n",
    "    for i in range(len(classNames)):\n",
    "        for j in range(len(classNames)):\n",
    "            plt.text(j,i, str(conf_matrix[i][j]), size='large', horizontalalignment='center')    \n",
    "\n",
    "gauss_NB_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 9.6> </a>Data Transformation\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerrit's data transformation\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Function to Transform variables using SKlearn PowerTransformer\n",
    "def transform_vars(data, features, method = 'yeo-johnson', standardize = True, rename_cols = 'Y'):\n",
    "    \n",
    "    # Save indices\n",
    "    indices = data.index\n",
    "    \n",
    "    # Fit the transformer\n",
    "    pt = PowerTransformer(method = method, standardize = standardize)\n",
    "    \n",
    "    # Transform Data\n",
    "    transformed_data = pd.DataFrame(pt.fit_transform(data[features])).set_index(indices)\n",
    "    \n",
    "    # Name columns\n",
    "    if rename_cols == 'Y':\n",
    "        transformed_data.columns = [f + '_transf' for f in features]\n",
    "    else: \n",
    "        transformed_data.columns = features\n",
    "        \n",
    "    return transformed_data\n",
    "\n",
    "continuous_features = train_data.columns[:10].tolist()\n",
    "\n",
    "# Find mean of each continuous column and append to list\n",
    "mean_list = []\n",
    "for feature in continuous_features:\n",
    "    mean_list.append(np.mean(train_data[feature]))\n",
    "    \n",
    "# Define function to binarize on the mean, credit Brenna in \"w207_final_project (1)\" notebook\n",
    "def binarize_data(data, feature_cols, thresholds = mean_list):\n",
    "    '''\n",
    "    Purpose: binarize continuous features, where 0 represents below (or equal to) mean, 1 represents above mean\n",
    "    Input: continuous feature columns, names of features to be binarized, thresholds for binarization\n",
    "    Output: binarized feature columns, same shape of input\n",
    "    '''\n",
    "    \n",
    "    # Initialize a new feature array with the same shape as the original data.\n",
    "    binarized_data = data.copy() # avoid changing the original data\n",
    "\n",
    "    # Apply a threshold  to each feature.\n",
    "    i = 0\n",
    "    for feature in feature_cols:\n",
    "        binarized_data[feature] = 1 * (binarized_data[feature] > thresholds[i])\n",
    "        i+=1\n",
    "    return binarized_data\n",
    "\n",
    "# Transform the data\n",
    "train_X_transf = transform_vars(data = train_data, features = continuous_features, \n",
    "               method = 'yeo-johnson', standardize = True, rename_cols = 'Y')\n",
    "dev_X_transf = transform_vars(data = dev_data, features = continuous_features, \n",
    "               method = 'yeo-johnson', standardize = True, rename_cols = 'Y')\n",
    "\n",
    "\n",
    "# Binarize the data\n",
    "train_X_transf = binarize_data(train_X_transf, train_X_transf.columns) # Binarize all continuous features\n",
    "dev_X_transf = binarize_data(dev_X_transf, train_X_transf.columns)\n",
    "\n",
    "# Combine the new, transformed columns with original DF\n",
    "train_X_full = train_data.merge(train_X_transf, how='left', on = train_data.index)\\\n",
    "                      .set_index(train_data.index)\\\n",
    "                      .drop(['key_0'], axis = 1)\n",
    "\n",
    "dev_X_full = dev_data.merge(dev_X_transf, how='left', on = dev_data.index)\\\n",
    "                  .set_index(dev_data.index)\\\n",
    "                  .drop(['key_0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Cutoffs: [0.25, 0.5, 0.75]\n",
      "Thresholds for Train Data:\n",
      " [[2376. 2752. 3103.]\n",
      " [  65.  126.  261.]\n",
      " [  10.   15.   22.]\n",
      " [ 767. 1318. 2274.]\n",
      " [ 731. 1253. 1987.]\n",
      " [  67.  187.  342.]\n",
      " [ 196.  220.  235.]\n",
      " [ 207.  223.  235.]\n",
      " [ 107.  138.  168.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brenna's data transformation\n",
    "\n",
    "# Creating new dataframes with original columns + Total_Distance_To_Hydrology\n",
    "train2 = train_data.copy()\n",
    "dev2 = dev_data.copy()\n",
    "\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "train2['Total_Distance_To_Hydrology'] = np.sqrt(train_data[\"Horizontal_Distance_To_Hydrology\"]**2 + train_data['Vertical_Distance_To_Hydrology']**2)\n",
    "dev2['Total_Distance_To_Hydrology'] = np.sqrt(dev_data[\"Horizontal_Distance_To_Hydrology\"]**2 + dev_data['Vertical_Distance_To_Hydrology']**2)\n",
    "\n",
    "# Splitting the data into two different dataframes:\n",
    "\n",
    "# Continuous Data Columns\n",
    "continuous_cols =  [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Roadways\",\n",
    "                   \"Horizontal_Distance_To_Fire_Points\", \"Total_Distance_To_Hydrology\",\n",
    "                   \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n",
    "\n",
    "# Binary / Categorical Data Columns\n",
    "categorical_cols = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "                   'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "                   'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "                   'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "                   'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "                   'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "                   'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "                   'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "                   'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "                   'Soil_Type39', 'Soil_Type40', 'Wilderness_Area1', 'Wilderness_Area2',\n",
    "                   'Wilderness_Area3', 'Wilderness_Area4']\n",
    "\n",
    "# Defining new train and dev data sets split by above lists\n",
    "# Continuous dataframes\n",
    "Xtrain_G = train2[continuous_cols]\n",
    "Xdev_G = dev2[continuous_cols]\n",
    "\n",
    "# Categorical dataframes\n",
    "Xtrain_C = train2[categorical_cols]\n",
    "Xdev_C = dev2[categorical_cols]\n",
    "\n",
    "# Creating thresholds based on quantiles\n",
    "\n",
    "def define_thresholds(data, num_divisions):\n",
    "    thresholds = np.zeros([len(data.columns), num_divisions-1])\n",
    "    step = round(1/num_divisions,2)\n",
    "    prev = 0\n",
    "    bins = []\n",
    "    for i in range(num_divisions-1):\n",
    "        num = prev\n",
    "        bins.append(num+step)\n",
    "        prev = num+step\n",
    "    print(\"Quantile Cutoffs:\", bins)\n",
    "    \n",
    "    i = 0\n",
    "    for item in Xtrain_G.columns:\n",
    "        for j in range(len(bins)):\n",
    "            thresholds[i][j] = int(data[item].quantile(bins[j]))\n",
    "        i+=1\n",
    "    return thresholds\n",
    "\n",
    "# threshold = NxM array, N = number of columns, M = number of bins\n",
    "# inputs: dataframe, threshold values\n",
    "# outputs: new dataframe\n",
    "def multifeature(data, thresholds):\n",
    "    # capture column names\n",
    "    features = list(data.columns)\n",
    "    \n",
    "    # initiate a new dataframe \n",
    "    new_df = data.copy()\n",
    "\n",
    "    i=0\n",
    "    # bin the data\n",
    "    for feature in features:\n",
    "        new_df[feature] = np.digitize(np.array(data[feature]), thresholds[i])\n",
    "        i+=1\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "num_divisions = 4\n",
    "thresholds = define_thresholds(Xtrain_G, num_divisions)\n",
    "print(\"Thresholds for Train Data:\\n\", thresholds, '\\n')\n",
    "train_df = multifeature(Xtrain_G, thresholds)\n",
    "dev_df = multifeature(Xdev_G, thresholds)\n",
    "\n",
    "#thresholds = define_thresholds(Xdev_G, num_divisions)\n",
    "#print(\"Thresholds for Dev Data:\\n\", thresholds)\n",
    "\n",
    "# This will merge our newly created multinomial features with the original binary features\n",
    "Xtrain_multi = pd.merge(train_df, Xtrain_C, left_on='Id', right_on='Id', how='left')\n",
    "\n",
    "Xdev_multi = pd.merge(dev_df, Xdev_C, left_on='Id', right_on='Id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2l7xYZubDt4"
   },
   "source": [
    "## <a id = 10> </a>Model Building\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "For the purposes of encapsulation and avoiding conflicts, each model cleaning/building is wrapped in a function. Eventually we will resolve conflicts and combine the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.scaler_type = None\n",
    "        self.X_train = None\n",
    "        self.X_dev = None\n",
    "        \n",
    "        if model_type == 'kNN':\n",
    "            self.model = KNeighborsClassifier()\n",
    "        elif model_type == 'Gaussian_NB':\n",
    "            self.model = GaussianNB()\n",
    "        elif model_type == 'Logistic_Regression':\n",
    "            self.model = LogisticRegression(random_state=0, max_iter=10000)\n",
    "        elif model_type == 'Decision_Tree':\n",
    "            self.model = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "        elif model_type == 'SVC':\n",
    "            self.model = SVC(random_state=0, kernel='rbf')\n",
    "        elif model_type == 'XGBoost':\n",
    "            self.model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=0)\n",
    "        elif model_type == 'Neural_Net':\n",
    "            self.model = MLPClassifier(random_state=0, max_iter=500)\n",
    "                  \n",
    "    def featurePreprocessingScale(self, scaler_type, X_train, X_dev):\n",
    "        self.scaler_type = scaler_type\n",
    "\n",
    "        if scaler_type == 'MinMax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == 'Standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'Robust':\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_dev_scaled = scaler.transform(X_dev)\n",
    "        \n",
    "        self.X_train_scaled = X_train_scaled\n",
    "        self.X_dev_scaled = X_dev_scaled\n",
    "\n",
    "        return([X_train_scaled, X_dev_scaled])\n",
    "    \n",
    "    def gridSearchCv(self, train_data, dev_data, train_labels, dev_labels,\n",
    "                     params=None, scaler_type=None):\n",
    "        \n",
    "        self.X_train = train_data\n",
    "        self.X_dev = dev_data        \n",
    "        \n",
    "        gscv = GridSearchCV(self.model, param_grid=params, cv=3, n_jobs=-1)\n",
    "        \n",
    "        if scaler_type != None:\n",
    "            [self.X_train, self.X_dev] = self.featurePreprocessingScale(scaler_type, train_data, dev_data)\n",
    "\n",
    "        gscv.fit(self.X_train, train_labels.values.ravel())\n",
    "        dev_predict = gscv.predict(self.X_dev)\n",
    "        \n",
    "        self.best_model = gscv\n",
    "        self.best_f1score = metrics.f1_score(dev_labels, dev_predict, average='weighted')\n",
    "        self.best_accuracy = metrics.accuracy_score(dev_labels, dev_predict)\n",
    "        self.dev_predict = dev_predict\n",
    "        self.classification_report = classification_report(dev_predict, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_options = [\n",
    "    'MinMax'\n",
    "    ,'Standard'\n",
    "    ,'Robust'\n",
    "    ,None\n",
    "]\n",
    "\n",
    "model_options = [\n",
    "    {'model_type':'kNN','params':{'n_neighbors':list(range(1, 3))}}\n",
    "#     ,{'model_type':'Gaussian_NB','params':{'var_smoothing':[0.001]}}\n",
    "#     ,{'model_type':'Logistic_Regression','params':{'C':[500, 1000]}}\n",
    "#     ,{'model_type':'Decision_Tree','params':{'max_leaf_nodes':[50]}}\n",
    "#     ,{'model_type':'SVC','params':{'C':[10],'gamma':[0.5]}}   \n",
    "#     ,{'model_type':'XGBoost','params':{'max_depth':[7],'subsample':[0.8],'n_estimators':[200]}}\n",
    "#     ,{'model_type':'Neural_Net','params':{'hidden_layer_sizes':[(100,),(100,20)]}}    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Model Number 1: kNN\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Best F1-Score: 0.7543\n",
      "        Best Accuracy: 0.7563\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 6.72s\n",
      "        \n",
      "\n",
      "        Model Number 2: kNN\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Best F1-Score: 0.7539\n",
      "        Best Accuracy: 0.7563\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 23.73s\n",
      "        \n",
      "\n",
      "        Model Number 3: kNN\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Best F1-Score: 0.7483\n",
      "        Best Accuracy: 0.7510\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 4.62s\n",
      "        \n",
      "\n",
      "        Model Number 4: kNN\n",
      "        Scaler Type: None\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Best F1-Score: 0.7361\n",
      "        Best Accuracy: 0.7394\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 6.55s\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Update as needed\n",
    "Xtrain = Xtrain_multi\n",
    "Xdev = Xdev_multi\n",
    "\n",
    "i = 1\n",
    "finalResult_df = pd.DataFrame()\n",
    "for scalerType in scaler_options:\n",
    "    for modelType in model_options:\n",
    "        start_time = time.time()\n",
    "        model = Model(model_type=modelType['model_type'])\n",
    "        model.gridSearchCv(Xtrain, Xdev, train_labels, dev_labels,\n",
    "                     params=modelType['params'], scaler_type=scalerType)\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "        f'''\n",
    "        Model Number {i}: {modelType['model_type']}\n",
    "        Scaler Type: {scalerType}\n",
    "        Parameters: {modelType['params']}\n",
    "        Best F1-Score: {model.best_f1score:.4f}\n",
    "        Best Accuracy: {model.best_accuracy:.4f}\n",
    "        Optimal Parameters: {model.best_model.best_params_}\n",
    "        Run Time: {end_time-start_time:.2f}s\n",
    "        '''\n",
    "        )\n",
    "        \n",
    "        finalResult_df = finalResult_df.append(pd.DataFrame(\n",
    "        {\n",
    "            'Model Number':[i]\n",
    "            ,'Model Type':[model.model_type]\n",
    "            ,'Scaler Type':[scalerType]\n",
    "            ,'F1-Score':[round(model.best_f1score, 4)]\n",
    "            , 'Accuracy':[round(model.best_accuracy, 4)]\n",
    "            ,'Optimal Parameters':[model.best_model.best_params_]\n",
    "            ,'Run Time (s)':[round(end_time-start_time, 2)]\n",
    "        }\n",
    "        )\n",
    "                                              )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 11> </a>Result Analyses\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Number</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Scaler Type</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Run Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kNN</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.7563</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kNN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.7539</td>\n",
       "      <td>0.7563</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>23.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>kNN</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.7483</td>\n",
       "      <td>0.7510</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>kNN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7361</td>\n",
       "      <td>0.7394</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>6.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Number Model Type Scaler Type  F1-Score  Accuracy  \\\n",
       "0             1        kNN      MinMax    0.7543    0.7563   \n",
       "1             2        kNN    Standard    0.7539    0.7563   \n",
       "2             3        kNN      Robust    0.7483    0.7510   \n",
       "3             4        kNN        None    0.7361    0.7394   \n",
       "\n",
       "   Optimal Parameters  Run Time (s)  \n",
       "0  {'n_neighbors': 1}          6.72  \n",
       "1  {'n_neighbors': 1}         23.73  \n",
       "2  {'n_neighbors': 1}          4.62  \n",
       "3  {'n_neighbors': 1}          6.55  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalResult_df.sort_values(by='F1-Score', ascending=False).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "w207 - final project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
