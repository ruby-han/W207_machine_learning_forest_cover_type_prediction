{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqnkWOMaA00A"
   },
   "source": [
    "# Forest Cover Type Prediction - Final Report\n",
    "The goal of this project is to build a model that uses cartographic features about a cell of forest land to accurately predict the predominant kind of tree cover for the cell.\n",
    "\n",
    "## The Inference Problem\n",
    "**X:** Cartographic fatures of forest land, such as elevation, slope, distance to water, shade, and soil type.\n",
    "\n",
    "**y:** Predominant class of tree cover for the given cell.\n",
    "\n",
    "**Model:** We will train a variety of models, including KNN, Naive Bayes, Decision Trees, and Others.\n",
    "\n",
    "**Parameters:** Each of the different models will encode the training data via the parameters. KNN is the exception, and no parameters will be stored.\n",
    "\n",
    "**Cost Functions:** Each model will employ a different cost function. For example, decision trees will use entropy.\n",
    "\n",
    "**Objective:** Each of the models will have their own objective, like maximizing likelihood, in the case of Naivve bayes.\n",
    "\n",
    "## [Data Source](https://www.kaggle.com/c/forest-cover-type-prediction)\n",
    "The outcome variable (forest cover type) comes from the US Forest Service, while the feature variables come from a combination of the US Geological survey as well as the USFS. This data encapsulates four wilderness areas in the Roosevelt National Forest; because these areas are preserved from most human disturbance, we assume forest cover types are a result of natural processes represented by the independent variables (although this assumption is not necessary to generate an effective model).\n",
    "\n",
    "## Feature Definitions\n",
    "The raw data contains a mixture of continuous and binary variables, as defined below: \n",
    "\n",
    "- `Elevation` - Elevation in meters\n",
    "- `Aspect` - Aspect in degrees azimuth\n",
    "- `Slope` - Slope in degrees\n",
    "- `Horizontal_Distance_To_Hydrology` - Horz Dist to nearest surface water features\n",
    "- `Vertical_Distance_To_Hydrology` - Vert Dist to nearest surface water features\n",
    "- `Horizontal_Distance_To_Roadways` - Horz Dist to nearest roadway\n",
    "- `Hillshade_9am` (0 to 255 index) - Hillshade index at 9am, summer solstice\n",
    "- `Hillshade_Noon` (0 to 255 index) - Hillshade index at noon, summer solstice\n",
    "- `Hillshade_3pm` (0 to 255 index) - Hillshade index at 3pm, summer solstice\n",
    "- `Horizontal_Distance_To_Fire_Points` - Horz Dist to nearest wildfire ignition points\n",
    "- `Wilderness_Area` (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n",
    "- `Soil_Type` (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n",
    "- `Cover_Type` (7 types, integers 1 to 7) - Forest Cover Type designation\n",
    "\n",
    "Additional transformed variables to potentially train our models:\n",
    "- `Wilderness_Area` - combining the 4 binary columns into one categorical variable (1-4), making an assumption about exclusivity of areas that needs to be checked\n",
    "- `Soil_Type` - combining the 40 binary columns into one categorical variable (1-40), making an assumption about exclusivity of soil types that needs to be checked\n",
    "- `Total_Distance_To_Hydrology` - Euclidean distance using \"Horizontal\" and \"Vertical\" distances\n",
    "- Binned versions of continuous variables\n",
    "\n",
    "## Testing Plan\n",
    "We plan to tune and compare a variety of models, optimizing toward the highest possible $F_1$ score (a metric which balances precision and recall). All models will be trained on labeled data, and tested against \"development\" data using a 50-50 split. \n",
    "\n",
    "Potential models to test include:\n",
    "- k Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machines\n",
    "\n",
    "## <a id = 0> </a>Navigation\n",
    "- [Data Load](#1)\n",
    "- [Data Split](#2)\n",
    "- [Exploratory Data Analyses](#3)\n",
    "    - [Histogram](#4)\n",
    "    - [Scatter Plots](#5)\n",
    "    - [Correlation Matrix](#6)\n",
    "    - [Box Plots](#7)\n",
    "    - [Violin Plots](#8)\n",
    "    - [Wilderness Area and Soil Types](#9)\n",
    "- [Confusion Matrix](#9.5)\n",
    "- [Model Building](#10)\n",
    "- [Result Analyses](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgCVuPjJ_DB_"
   },
   "source": [
    "## <a id = 1> </a> Data Load\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "CVzHgUNb_DCC"
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn - learning libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# SK-learn - feature processing libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# SK-learn - evaluation libraries\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Producing Decision Tree diagrams\n",
    "from IPython.display import Image, display\n",
    "import pydotplus\n",
    "from subprocess import call\n",
    "\n",
    "# Other\n",
    "import copy\n",
    "from textwrap import wrap\n",
    "\n",
    "# Expand rows/columns in df outputs\n",
    "pd.set_option(\n",
    "#     'max_rows', None, \n",
    "    'max_columns', None,\n",
    "    'max_colwidth', None\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 2> </a> Data Split\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/processed/train_data.csv').set_index('Id')\n",
    "train_labels = pd.read_csv('../data/processed/train_labels.csv').set_index('Id')\n",
    "dev_data = pd.read_csv('../data/processed/dev_data.csv').set_index('Id')\n",
    "dev_labels = pd.read_csv('../data/processed/dev_labels.csv').set_index('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (12096, 54)\n",
      "Train Labels Shape: (12096, 1)\n",
      "Dev Data Shape: (3024, 54)\n",
      "Dev Labels Shape: (3024, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Train Data Shape: {train_data.shape}'\n",
    "    f'\\nTrain Labels Shape: {train_labels.shape}'\n",
    "    f'\\nDev Data Shape: {dev_data.shape}'\n",
    "    f'\\nDev Labels Shape: {dev_labels.shape}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwr7mLW_LOoq"
   },
   "source": [
    "## <a id = 3> </a>Exploratory Data Analyses\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "d_agWMq__DCE",
    "outputId": "a314c7e5-3e96-48db-d6f6-9e6807969937"
   },
   "outputs": [],
   "source": [
    "# Column List\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "collapsed": true,
    "id": "OvljDYI_7cdU",
    "outputId": "dc55a1b9-ce82-4864-81e6-3c84fd0dceee"
   },
   "outputs": [],
   "source": [
    "# Statistics Summary\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check data types for each field\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check null values\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- All data fields are int64 objects\n",
    "- `Wilderness_Area` and `Soil_Type` are binary features\n",
    "- `Cover_Type` is categorized from 1-7\n",
    "- The rest of the fields are continuous\n",
    "- No null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bcQ8miO9RdS"
   },
   "source": [
    "### <a id = 4> </a>Histograms of each non-binary feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Given Random split between train and dev, we would expect the training distributions to compare similarly to our dev data. This will be key in in generalization both across Dev data, as well as final test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "collapsed": true,
    "id": "d1yYGw5M9qDF",
    "outputId": "e409242a-a078-4d7c-856d-6f29edf11fd1"
   },
   "outputs": [],
   "source": [
    "# Note: When you export this notebook and run it in Jupyter Lab, you need to\n",
    "# reset the first column of test_kaggle as you did with train and test:\n",
    "# test_data = test_kaggle.set_index('Id')\n",
    "# Otherwise, the last row of graphs are one off\n",
    "\n",
    "# Strip underscores from feature names for nice printing\n",
    "formatted_cols = copy.deepcopy(X_train_df.columns).str.replace('_', ' ')\n",
    "\n",
    "# Plot Formatting\n",
    "plt.rcParams.update({'text.color' : \"dimgrey\",\n",
    "                     'axes.labelcolor' : \"grey\"})\n",
    "\n",
    "# include dev_data in plots for comparison\n",
    "# datasets = [train_data, dev_data]    \n",
    "# data_names = ['train', 'dev']\n",
    "\n",
    "datasets = [X_train_df]\n",
    "data_names = ['train']\n",
    "\n",
    "# For Train, Dev, and Test, plot each non-binary feature\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through to show hist of non-binary for each \n",
    "for d, data in enumerate(datasets):    # For each dataset (only needed when comparing dev and train)\n",
    "    for i in np.arange(0, 10):          # For each non-binary figure in dataset\n",
    "\n",
    "        \n",
    "        \n",
    "        data.iloc[:, i].plot.hist(ax = axes[i], \n",
    "                                    figsize = (20,5), \n",
    "                                    sharex = True, color = '#1c4966')\n",
    "\n",
    "        \n",
    "        # Column and Row names for each plot\n",
    "        if (i == 0) and (d == 0):    # Top Left Corner\n",
    "            axes[i].set_ylabel(data_names[d])\n",
    "            axes[i].set_title(\"\\n\".join(wrap(formatted_cols[i], 12)))\n",
    "        \n",
    "        elif i == 0:    # First Column\n",
    "            axes[i].set_ylabel(data_names[d])\n",
    "    \n",
    "        elif d == 0:    # First Row\n",
    "            axes[i].set_ylabel('')\n",
    "            axes[i].set_title(\"\\n\".join(wrap(formatted_cols[i], 12)))\n",
    "        else:\n",
    "            axes[i].set_ylabel('')\n",
    "            \n",
    "        # For All Plots\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_color('grey')\n",
    "        axes[i].spines['bottom'].set_color('grey')\n",
    "        axes[i].tick_params(colors = 'grey')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbAfF1cMn57"
   },
   "source": [
    "### <a id = 5> </a> Scatterplots comparing each feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Scatterplots may reveal correlational relationships between features. Additionally, the color of each datapoint represents a forest cover type. This will also help reveal if the relationship between two features varies by forest cover type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "collapsed": true,
    "id": "rUD1ZQl8M2A5",
    "outputId": "7787543d-932f-49a0-a9af-006d5edbe618"
   },
   "outputs": [],
   "source": [
    "# Scatterplot Matrix\n",
    "# ------------------------------------------------------------------------------\n",
    "# This isn't meant to be a final output (obviously it's too much in its current\n",
    "# state); just wanted to see all of the distributions at once so we could pick\n",
    "# out meaningful ones \n",
    "# Currently, this takes a long time to run.\n",
    "\n",
    "train_data_copy = X_train_df.copy().iloc[:, :10]\n",
    "train_data_copy[\"Cover_Type\"] = X_train_df.Cover_Type\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "collapsed": true,
    "id": "Flgtn5cXCGYw",
    "outputId": "94297b80-7f9e-4ea7-e306-c1acaf0e6444"
   },
   "outputs": [],
   "source": [
    "# Cutting down the number of columns\n",
    "columns = [\"Elevation\", \"Aspect\", \"Slope\", \"Hillshade_9am\",\n",
    "           \"Hillshade_Noon\", \"Hillshade_3pm\", \"Cover_Type\"]\n",
    "\n",
    "train_data_copy2 = X_train_df.copy().loc[:, columns]\n",
    "train_data_copy2\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy2, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "collapsed": true,
    "id": "Hs3lJQDLCG9p",
    "outputId": "2db785fa-1413-492c-cc05-9e1bd2feaa63"
   },
   "outputs": [],
   "source": [
    "# Rest of the columns\n",
    "columns = [\"Horizontal_Distance_To_Hydrology\",\n",
    "           \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "           \"Horizontal_Distance_To_Fire_Points\", \"Hillshade_9am\",\n",
    "           \"Hillshade_Noon\", \"Hillshade_3pm\", \"Cover_Type\"]\n",
    "\n",
    "train_data_copy3 = X_train_df.copy().loc[:, columns]\n",
    "train_data_copy3\n",
    "\n",
    "# The different colors indicate Cover_Type\n",
    "sns.pairplot(train_data_copy3, kind=\"scatter\", hue=\"Cover_Type\", palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOOnDjwXMzAa"
   },
   "source": [
    "### <a id = 6> </a>Correlation Matrix - Relationships between each non-binary feature\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Comparing the train_data heatmap to dev_data, it is evident that they have largely the same correlation structure. This is expected given the random 80/20 split, but it is important to note any deviations in structure will lead to poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "collapsed": true,
    "id": "b6ZXlw7cNYJI",
    "outputId": "bde80405-f95d-477b-d2ed-c930758e6c6d"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharey = True, figsize = (20,10))\n",
    "\n",
    "datasets = [X_train_df, X_dev_df]    \n",
    "data_names = ['train', 'dev']\n",
    "\n",
    "# Correlation plot for each dataset - numeric values\n",
    "for i, data in enumerate(datasets):    # For each dataset\n",
    "\n",
    "    corr = data.iloc[:, :10].corr()    # Set the correlation matrix\n",
    "    \n",
    "    # Mask to upper triangular\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    sns.heatmap(corr, \n",
    "                xticklabels = corr.columns.values,\n",
    "                yticklabels = corr.columns.values, \n",
    "                cmap = 'bwr', \n",
    "                annot = True, \n",
    "                mask = mask, \n",
    "                fmt = '.2f',\n",
    "                ax = axes[i],\n",
    "                cbar = False).set(title = data_names[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 7> </a>Boxplots for Numeric Features\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Cartographic features like Elevation, Aspect, and Slope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 1, figsize = (20, 35))\n",
    "\n",
    "feature_cols = [\"Elevation\", \"Aspect\", \"Slope\", \n",
    "                \"Horizontal_Distance_To_Roadways\", \n",
    "                \"Horizontal_Distance_To_Fire_Points\", \"Hillshade_9am\",\n",
    "                \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "                \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\"] # Added these to feature_cols\n",
    "\n",
    "for i, var in enumerate(feature_cols):\n",
    "    sns.boxplot(x = var, data = X_train_df, ax = ax[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 8> </a>Violin Plot on Continuous Features\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Violin plot\n",
    "fig, axes = plt.subplots(5, 2, figsize = (20, 20))\n",
    "col_list = ['Elevation', 'Aspect', 'Slope',\n",
    "            'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology', \n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am',\n",
    "            'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\n",
    "i = 0\n",
    "for col_name in col_list:\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    sns.violinplot(x='Cover_Type', y=col_name, data=X_train_df , ax=axes[row][col])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- `Elevation` varies according to `Cover_Type` indicating that this will be an important variable for prediction\n",
    "- `Horizontal_Distance_To_Hydrology` and `Horizontal_Distance_To_Roadways` have similar distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 9> </a>`Wilderness_Area` and `Soil_Type` Binary Features Exploration\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Unpivot `Wilderness_Area` and `Soil_Type` Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_list = []\n",
    "for i in range(40):\n",
    "    soil_list.append(f'Soil_Type{i+1}')\n",
    "\n",
    "wild_area_list = ['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unpivot df from wide to long format by combining `Soil_Type#` and `Wilderness_Area#` to one column each\n",
    "X_train_df_comb = pd.melt(X_train_df, \n",
    "                          id_vars=col_list+soil_list+['Cover_Type'], \n",
    "                          value_vars=wild_area_list, \n",
    "                          var_name='Wilderness_Area')\n",
    "X_train_df_comb2 = X_train_df_comb[X_train_df_comb.value != 0].drop(columns=['value'])\n",
    "\n",
    "X_train_df_comb3 = pd.melt(X_train_df_comb2, \n",
    "                          id_vars=col_list+['Wilderness_Area', 'Cover_Type'], \n",
    "                          value_vars=soil_list, \n",
    "                          var_name='Soil_Type')\n",
    "X_train_df_comb4 = X_train_df_comb3[X_train_df_comb3.value != 0].drop(columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count plot - Combined Wilderness Area\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.countplot(x='Wilderness_Area', hue='Cover_Type', data=X_train_df_comb4)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x()-0.001, p.get_height()+10))\n",
    "plt.legend(loc='upper right', title='Cover Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Cover Type 4 only exists in Wilderness Area 4\n",
    "- Fairly equal representation of wilderness areas, except for Wilderness Area 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count plot - Soil Type\n",
    "plt.figure(figsize=(50, 10))\n",
    "ax = sns.countplot(x='Soil_Type', hue='Cover_Type', data=X_train_df_comb4)\n",
    "# for p in ax.patches:\n",
    "#     ax.annotate(f'{p.get_height():.0f}', (p.get_x(), p.get_height()+10))\n",
    "plt.legend(loc='upper right', title='Cover Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- There are no cover types for Soil Type 7 and 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys_r_uq2NpWm"
   },
   "source": [
    "### Explore Wilderness Area Binary Counts\n",
    "Fairly equal representation of wilderness areas, except for Wilderness Area 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kLX-cerx_DCF",
    "outputId": "e0e331da-20b3-4ab2-e53d-f8cac5ac1cdd"
   },
   "outputs": [],
   "source": [
    "X_train_df.groupby(['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'])['Cover_Type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzgc_iuUN31T"
   },
   "source": [
    "### Determining if one Soil type exists for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ieqRiz3g7lEe",
    "outputId": "ca72ed18-9758-4ebc-814b-63c45bc7e1d8"
   },
   "outputs": [],
   "source": [
    "# Determining if one soil type exists for each data point\n",
    "soil_type_cols = [col_name for col_name in X_train_df.columns if 'Soil_Type' in col_name]\n",
    "\n",
    "X_train_df_soil = X_train_df.copy()\n",
    "X_train_df_soil['Soil_Type_Count'] = X_train_df_soil[soil_type_cols].sum(axis = 1)\n",
    "X_train_df_soil['Soil_Type_Count'].value_counts()\n",
    "\n",
    "# Only 1 soil type exists for each row - no mix of different soil types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YaNm4CEDUNSD",
    "outputId": "a2ce4b33-bc59-4dbd-b8af-78851dbe963a"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     dev_data.drop(columns = 'Soil_Type_Count', axis = 1)\n",
    "# except:\n",
    "#     print('Soil_Type_Count not yet created')\n",
    "\n",
    "X_dev_df_soil = X_dev_df.copy()\n",
    "X_dev_df_soil['Soil_Type_Count'] = X_dev_df_soil[soil_type_cols].sum(axis = 1)\n",
    "X_dev_df_soil['Soil_Type_Count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-5xnoMY8O_y"
   },
   "source": [
    "\n",
    "Looks like both Soil_Types and Wilderness_Areas are mutually exclusive within the columns (only 1 area/type per row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "FV5Zpmf8am45"
   },
   "outputs": [],
   "source": [
    "# Wilderness Areas and Soil Types\n",
    "# ------------------------------------------------------------------------------\n",
    "# Combining wilderness areas into one column, soil types into one column\n",
    "\n",
    "def get_feature_number(r, col_prefix):\n",
    "  \n",
    "  # gets the column name suffix of the true variable (wilderness area/soil type)\n",
    "    cols = [col_name for col_name in r.index if (col_prefix in col_name) and re.search(r'\\d', col_name) is not None]\n",
    "\n",
    "\n",
    "    feature_subix = r[cols].argmax()\n",
    "\n",
    "  #\n",
    "    feature_name = cols[feature_subix]\n",
    "    feature_num = ''.join([i for i in feature_name if i.isdigit()])\n",
    "    return int(feature_num)\n",
    "\n",
    "X_train_df_DR = X_train_df_soil.copy()\n",
    "X_dev_df_DR = X_dev_df_soil.copy()\n",
    "\n",
    "X_train_df_DR['Wilderness_Area'] = X_train_df_DR.apply(lambda x:get_feature_number(x, 'Wilderness_Area'), axis = 1)\n",
    "X_train_df_DR['Soil_Type'] = X_train_df_DR.apply(lambda x:get_feature_number(x, 'Soil_Type'), axis = 1)\n",
    "X_dev_df_DR['Wilderness_Area'] = X_dev_df_DR.apply(lambda x:get_feature_number(x, 'Wilderness_Area'), axis = 1)\n",
    "X_dev_df_DR['Soil_Type'] = X_dev_df_DR.apply(lambda x:get_feature_number(x, 'Soil_Type'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total_Distance_to_Hydrology\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create Total_Distance_to_Hydrology based on Euclidean distance\n",
    "X_train_df_DR['Total_Distance_To_Hydrology'] = np.sqrt(X_train_df_DR[\"Horizontal_Distance_To_Hydrology\"]**2 + X_train_df_DR['Vertical_Distance_To_Hydrology']**2)\n",
    "X_dev_df_DR['Total_Distance_To_Hydrology'] = np.sqrt(X_dev_df_DR[\"Horizontal_Distance_To_Hydrology\"]**2 + X_dev_df_DR['Vertical_Distance_To_Hydrology']**2)\n",
    "X_train_df_DR[[\"Total_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "b_7oD2jp3dV3",
    "outputId": "e292ed4c-db2d-4366-d2d7-2785d4ad42da"
   },
   "outputs": [],
   "source": [
    "X_train_df_DR['Wilderness_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FIdJQPP08UNE",
    "outputId": "ff71aa54-4209-4a3f-df39-89685aee2a6e"
   },
   "outputs": [],
   "source": [
    "X_train_df_DR['Soil_Type'].value_counts().sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 9.5> </a>Confusion Matrix \n",
    "[Back to Navigation](#0)\n",
    "\n",
    "Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss_NB_confusion_matrix():\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(train_data.iloc[:, :10])\n",
    "    X_dev_std = scaler.transform(dev_data.iloc[:, :10])\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train_std, train_labels.values.ravel())\n",
    "    dev_pred = model.predict(X_dev_std)\n",
    "    \n",
    "    nb_f1_score = metrics.f1_score(dev_pred, dev_labels, average = 'weighted')\n",
    "    \n",
    "    print(f'Gaussian NB f1_score: {nb_f1_score:.4f}\\n')\n",
    "    \n",
    "    # Print confusion matrix in ASCII form\n",
    "    conf_matrix = confusion_matrix(dev_labels, dev_pred)\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Produce confusion matrix in the form of heatmap\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "    cmx = ax.matshow(conf_matrix, cmap=plt.cm.Accent)\n",
    "    plt.colorbar(cmx)\n",
    "    \n",
    "    plt.title('Confusion Matrix Heat Map')\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Accent)\n",
    "    classNames = [str(i+1) for i in range(conf_matrix.shape[0])]    \n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=0, fontsize=14)\n",
    "    plt.yticks(tick_marks, classNames, fontsize=14)\n",
    "   \n",
    "    for i in range(len(classNames)):\n",
    "        for j in range(len(classNames)):\n",
    "            plt.text(j,i, str(conf_matrix[i][j]), size='large', horizontalalignment='center')    \n",
    "\n",
    "gauss_NB_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2l7xYZubDt4"
   },
   "source": [
    "## <a id = 10> </a>Model Building\n",
    "[Back to Navigation](#0)\n",
    "\n",
    "For the purposes of encapsulation and avoiding conflicts, each model cleaning/building is wrapped in a function. Eventually we will resolve conflicts and combine the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.scaler_type = None\n",
    "        self.X_train = None\n",
    "        self.X_dev = None\n",
    "        \n",
    "        if model_type == 'kNN':\n",
    "            self.model = KNeighborsClassifier()\n",
    "        elif model_type == 'Gaussian_NB':\n",
    "            self.model = GaussianNB()\n",
    "        elif model_type == 'Logistic_Regression':\n",
    "            self.model = LogisticRegression(random_state=0, max_iter=10000)\n",
    "        elif model_type == 'Decision_Tree':\n",
    "            self.model = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "        elif model_type == 'SVC':\n",
    "            self.model = SVC(random_state=0, kernel='rbf')\n",
    "        elif model_type == 'XGBoost':\n",
    "            self.model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=0)\n",
    "        elif model_type == 'Neural_Net':\n",
    "            self.model = MLPClassifier(random_state=0, max_iter=500)\n",
    "                  \n",
    "    def featurePreprocessingScale(self, scaler_type, X_train, X_dev):\n",
    "        self.scaler_type = scaler_type\n",
    "\n",
    "        if scaler_type == 'MinMax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == 'Standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'Robust':\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_dev_scaled = scaler.transform(X_dev)\n",
    "        \n",
    "        self.X_train_scaled = X_train_scaled\n",
    "        self.X_dev_scaled = X_dev_scaled\n",
    "\n",
    "        return([X_train_scaled, X_dev_scaled])\n",
    "    \n",
    "    def gridSearchCv(self, train_data, dev_data, train_labels, dev_labels,\n",
    "                     params=None, scaler_type=None):\n",
    "        \n",
    "        self.X_train = train_data\n",
    "        self.X_dev = dev_data        \n",
    "        \n",
    "        gscv = GridSearchCV(self.model, param_grid=params, cv=3, n_jobs=-1)\n",
    "        \n",
    "        if scaler_type != None:\n",
    "            [self.X_train, self.X_dev] = self.featurePreprocessingScale(scaler_type, train_data, dev_data)\n",
    "\n",
    "        gscv.fit(self.X_train, train_labels.values.ravel())\n",
    "        dev_predict = gscv.predict(self.X_dev)\n",
    "        \n",
    "        self.best_model = gscv\n",
    "        self.best_f1score = metrics.f1_score(dev_labels, dev_predict, average='weighted')\n",
    "        self.best_accuracy = metrics.accuracy_score(dev_labels, dev_predict)\n",
    "        self.dev_predict = dev_predict\n",
    "        self.classification_report = classification_report(dev_predict, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_options = ['MinMax', 'Standard', 'Robust', None]\n",
    "model_options = [\n",
    "    {'model_type':'kNN','params':{'n_neighbors':list(range(1, 3))}}\n",
    "    ,{'model_type':'Gaussian_NB','params':{'var_smoothing':[0.001]}}\n",
    "    ,{'model_type':'Logistic_Regression','params':{'C':[500, 1000]}}\n",
    "    ,{'model_type':'Decision_Tree','params':{'max_leaf_nodes':[50]}}\n",
    "    ,{'model_type':'SVC','params':{'C':[10],'gamma':[0.5]}}   \n",
    "    ,{'model_type':'XGBoost','params':{'max_depth':[7],'subsample':[0.8],'n_estimators':[200]}}\n",
    "    ,{'model_type':'Neural_Net','params':{'hidden_layer_sizes':[(100,),(100,20)]}}    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Model Number 1: kNN\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Highest F1-Score: 0.8171\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 9.72s\n",
      "        \n",
      "\n",
      "        Model Number 2: Gaussian_NB\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'var_smoothing': [0.001]}\n",
      "        Highest F1-Score: 0.5032\n",
      "        Optimal Parameters: {'var_smoothing': 0.001}\n",
      "        Run Time: 0.29s\n",
      "        \n",
      "\n",
      "        Model Number 3: Logistic_Regression\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'C': [500, 1000]}\n",
      "        Highest F1-Score: 0.6985\n",
      "        Optimal Parameters: {'C': 1000}\n",
      "        Run Time: 63.25s\n",
      "        \n",
      "\n",
      "        Model Number 4: Decision_Tree\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'max_leaf_nodes': [50]}\n",
      "        Highest F1-Score: 0.6908\n",
      "        Optimal Parameters: {'max_leaf_nodes': 50}\n",
      "        Run Time: 0.48s\n",
      "        \n",
      "\n",
      "        Model Number 5: SVC\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'C': [10], 'gamma': [0.5]}\n",
      "        Highest F1-Score: 0.7859\n",
      "        Optimal Parameters: {'C': 10, 'gamma': 0.5}\n",
      "        Run Time: 16.13s\n",
      "        \n",
      "\n",
      "        Model Number 6: XGBoost\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'max_depth': [7], 'subsample': [0.8], 'n_estimators': [200]}\n",
      "        Highest F1-Score: 0.8701\n",
      "        Optimal Parameters: {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        Run Time: 129.84s\n",
      "        \n",
      "\n",
      "        Model Number 7: Neural_Net\n",
      "        Scaler Type: MinMax\n",
      "        Parameters: {'hidden_layer_sizes': [(100,), (100, 20)]}\n",
      "        Highest F1-Score: 0.8205\n",
      "        Optimal Parameters: {'hidden_layer_sizes': (100, 20)}\n",
      "        Run Time: 178.33s\n",
      "        \n",
      "\n",
      "        Model Number 8: kNN\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Highest F1-Score: 0.8009\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 13.65s\n",
      "        \n",
      "\n",
      "        Model Number 9: Gaussian_NB\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'var_smoothing': [0.001]}\n",
      "        Highest F1-Score: 0.4532\n",
      "        Optimal Parameters: {'var_smoothing': 0.001}\n",
      "        Run Time: 0.32s\n",
      "        \n",
      "\n",
      "        Model Number 10: Logistic_Regression\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'C': [500, 1000]}\n",
      "        Highest F1-Score: 0.6978\n",
      "        Optimal Parameters: {'C': 1000}\n",
      "        Run Time: 10.50s\n",
      "        \n",
      "\n",
      "        Model Number 11: Decision_Tree\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'max_leaf_nodes': [50]}\n",
      "        Highest F1-Score: 0.6908\n",
      "        Optimal Parameters: {'max_leaf_nodes': 50}\n",
      "        Run Time: 0.58s\n",
      "        \n",
      "\n",
      "        Model Number 12: SVC\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'C': [10], 'gamma': [0.5]}\n",
      "        Highest F1-Score: 0.8310\n",
      "        Optimal Parameters: {'C': 10, 'gamma': 0.5}\n",
      "        Run Time: 44.90s\n",
      "        \n",
      "\n",
      "        Model Number 13: XGBoost\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'max_depth': [7], 'subsample': [0.8], 'n_estimators': [200]}\n",
      "        Highest F1-Score: 0.8667\n",
      "        Optimal Parameters: {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        Run Time: 129.32s\n",
      "        \n",
      "\n",
      "        Model Number 14: Neural_Net\n",
      "        Scaler Type: Standard\n",
      "        Parameters: {'hidden_layer_sizes': [(100,), (100, 20)]}\n",
      "        Highest F1-Score: 0.8351\n",
      "        Optimal Parameters: {'hidden_layer_sizes': (100, 20)}\n",
      "        Run Time: 196.52s\n",
      "        \n",
      "\n",
      "        Model Number 15: kNN\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Highest F1-Score: 0.8023\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 4.57s\n",
      "        \n",
      "\n",
      "        Model Number 16: Gaussian_NB\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'var_smoothing': [0.001]}\n",
      "        Highest F1-Score: 0.5250\n",
      "        Optimal Parameters: {'var_smoothing': 0.001}\n",
      "        Run Time: 0.40s\n",
      "        \n",
      "\n",
      "        Model Number 17: Logistic_Regression\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'C': [500, 1000]}\n",
      "        Highest F1-Score: 0.6978\n",
      "        Optimal Parameters: {'C': 1000}\n",
      "        Run Time: 29.29s\n",
      "        \n",
      "\n",
      "        Model Number 18: Decision_Tree\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'max_leaf_nodes': [50]}\n",
      "        Highest F1-Score: 0.6908\n",
      "        Optimal Parameters: {'max_leaf_nodes': 50}\n",
      "        Run Time: 0.50s\n",
      "        \n",
      "\n",
      "        Model Number 19: SVC\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'C': [10], 'gamma': [0.5]}\n",
      "        Highest F1-Score: 0.8461\n",
      "        Optimal Parameters: {'C': 10, 'gamma': 0.5}\n",
      "        Run Time: 22.99s\n",
      "        \n",
      "\n",
      "        Model Number 20: XGBoost\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'max_depth': [7], 'subsample': [0.8], 'n_estimators': [200]}\n",
      "        Highest F1-Score: 0.8689\n",
      "        Optimal Parameters: {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        Run Time: 117.44s\n",
      "        \n",
      "\n",
      "        Model Number 21: Neural_Net\n",
      "        Scaler Type: Robust\n",
      "        Parameters: {'hidden_layer_sizes': [(100,), (100, 20)]}\n",
      "        Highest F1-Score: 0.8436\n",
      "        Optimal Parameters: {'hidden_layer_sizes': (100, 20)}\n",
      "        Run Time: 194.28s\n",
      "        \n",
      "\n",
      "        Model Number 22: kNN\n",
      "        Scaler Type: None\n",
      "        Parameters: {'n_neighbors': [1, 2]}\n",
      "        Highest F1-Score: 0.8474\n",
      "        Optimal Parameters: {'n_neighbors': 1}\n",
      "        Run Time: 1.48s\n",
      "        \n",
      "\n",
      "        Model Number 23: Gaussian_NB\n",
      "        Scaler Type: None\n",
      "        Parameters: {'var_smoothing': [0.001]}\n",
      "        Highest F1-Score: 0.5763\n",
      "        Optimal Parameters: {'var_smoothing': 0.001}\n",
      "        Run Time: 0.28s\n",
      "        \n",
      "\n",
      "        Model Number 24: Logistic_Regression\n",
      "        Scaler Type: None\n",
      "        Parameters: {'C': [500, 1000]}\n",
      "        Highest F1-Score: 0.6715\n",
      "        Optimal Parameters: {'C': 1000}\n",
      "        Run Time: 275.00s\n",
      "        \n",
      "\n",
      "        Model Number 25: Decision_Tree\n",
      "        Scaler Type: None\n",
      "        Parameters: {'max_leaf_nodes': [50]}\n",
      "        Highest F1-Score: 0.6908\n",
      "        Optimal Parameters: {'max_leaf_nodes': 50}\n",
      "        Run Time: 0.48s\n",
      "        \n",
      "\n",
      "        Model Number 26: SVC\n",
      "        Scaler Type: None\n",
      "        Parameters: {'C': [10], 'gamma': [0.5]}\n",
      "        Highest F1-Score: 0.0350\n",
      "        Optimal Parameters: {'C': 10, 'gamma': 0.5}\n",
      "        Run Time: 58.10s\n",
      "        \n",
      "\n",
      "        Model Number 27: XGBoost\n",
      "        Scaler Type: None\n",
      "        Parameters: {'max_depth': [7], 'subsample': [0.8], 'n_estimators': [200]}\n",
      "        Highest F1-Score: 0.8654\n",
      "        Optimal Parameters: {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        Run Time: 134.11s\n",
      "        \n",
      "\n",
      "        Model Number 28: Neural_Net\n",
      "        Scaler Type: None\n",
      "        Parameters: {'hidden_layer_sizes': [(100,), (100, 20)]}\n",
      "        Highest F1-Score: 0.6593\n",
      "        Optimal Parameters: {'hidden_layer_sizes': (100, 20)}\n",
      "        Run Time: 26.56s\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "finalResult_df = pd.DataFrame()\n",
    "for scalerType in scaler_options:\n",
    "    for modelType in model_options:\n",
    "        start_time = time.time()\n",
    "        model = Model(model_type=modelType['model_type'])\n",
    "        model.gridSearchCv(train_data, dev_data, train_labels, dev_labels,\n",
    "                     params=modelType['params'], scaler_type=scalerType)\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "        f'''\n",
    "        Model Number {i}: {modelType['model_type']}\n",
    "        Scaler Type: {scalerType}\n",
    "        Parameters: {modelType['params']}\n",
    "        Highest F1-Score: {model.best_f1score:.4f}\n",
    "        Highest Accuracy: {model.best_accuracy:.4f}\n",
    "        Optimal Parameters: {model.best_model.best_params_}\n",
    "        Run Time: {end_time-start_time:.2f}s\n",
    "        '''\n",
    "        )\n",
    "        \n",
    "        finalResult_df = finalResult_df.append(pd.DataFrame(\n",
    "        {\n",
    "            'Model Number':[i]\n",
    "            ,'Model Type':[model.model_type]\n",
    "            ,'Scaler Type':[scalerType]\n",
    "            ,'F1-Score':[round(model.best_f1score, 4)]\n",
    "            ,'Optimal Parameters':[model.best_model.best_params_]\n",
    "            ,'Run Time':[round(end_time-start_time, 2)]\n",
    "        }\n",
    "        )\n",
    "                                              )\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 11> </a>Result Analyses\n",
    "[Back to Navigation](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Number</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Scaler Type</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.8667</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8654</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>kNN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8474</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>SVC</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.8461</td>\n",
       "      <td>{'C': 10, 'gamma': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>Neural_Net</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 20)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>Neural_Net</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 20)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>SVC</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.8310</td>\n",
       "      <td>{'C': 10, 'gamma': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Neural_Net</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.8205</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 20)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>kNN</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.8171</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>kNN</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.8023</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>kNN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.8009</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>SVC</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>{'C': 10, 'gamma': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6985</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.6978</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.6978</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>Decision_Tree</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>{'max_leaf_nodes': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Decision_Tree</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>{'max_leaf_nodes': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>Decision_Tree</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>{'max_leaf_nodes': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>Decision_Tree</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>{'max_leaf_nodes': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24</td>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>Neural_Net</td>\n",
       "      <td>None</td>\n",
       "      <td>0.6593</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 20)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Gaussian_NB</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5763</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16</td>\n",
       "      <td>Gaussian_NB</td>\n",
       "      <td>Robust</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>Gaussian_NB</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>0.5032</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9</td>\n",
       "      <td>Gaussian_NB</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0.4532</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>SVC</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>{'C': 10, 'gamma': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model Number           Model Type Scaler Type  F1-Score  \\\n",
       "0              6              XGBoost      MinMax    0.8701   \n",
       "1             20              XGBoost      Robust    0.8689   \n",
       "2             13              XGBoost    Standard    0.8667   \n",
       "3             27              XGBoost        None    0.8654   \n",
       "4             22                  kNN        None    0.8474   \n",
       "5             19                  SVC      Robust    0.8461   \n",
       "6             21           Neural_Net      Robust    0.8436   \n",
       "7             14           Neural_Net    Standard    0.8351   \n",
       "8             12                  SVC    Standard    0.8310   \n",
       "9              7           Neural_Net      MinMax    0.8205   \n",
       "10             1                  kNN      MinMax    0.8171   \n",
       "11            15                  kNN      Robust    0.8023   \n",
       "12             8                  kNN    Standard    0.8009   \n",
       "13             5                  SVC      MinMax    0.7859   \n",
       "14             3  Logistic_Regression      MinMax    0.6985   \n",
       "15            10  Logistic_Regression    Standard    0.6978   \n",
       "16            17  Logistic_Regression      Robust    0.6978   \n",
       "17            11        Decision_Tree    Standard    0.6908   \n",
       "18            18        Decision_Tree      Robust    0.6908   \n",
       "19             4        Decision_Tree      MinMax    0.6908   \n",
       "20            25        Decision_Tree        None    0.6908   \n",
       "21            24  Logistic_Regression        None    0.6715   \n",
       "22            28           Neural_Net        None    0.6593   \n",
       "23            23          Gaussian_NB        None    0.5763   \n",
       "24            16          Gaussian_NB      Robust    0.5250   \n",
       "25             2          Gaussian_NB      MinMax    0.5032   \n",
       "26             9          Gaussian_NB    Standard    0.4532   \n",
       "27            26                  SVC        None    0.0350   \n",
       "\n",
       "                                         Optimal Parameters  \n",
       "0   {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}  \n",
       "1   {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}  \n",
       "2   {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}  \n",
       "3   {'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}  \n",
       "4                                        {'n_neighbors': 1}  \n",
       "5                                   {'C': 10, 'gamma': 0.5}  \n",
       "6                         {'hidden_layer_sizes': (100, 20)}  \n",
       "7                         {'hidden_layer_sizes': (100, 20)}  \n",
       "8                                   {'C': 10, 'gamma': 0.5}  \n",
       "9                         {'hidden_layer_sizes': (100, 20)}  \n",
       "10                                       {'n_neighbors': 1}  \n",
       "11                                       {'n_neighbors': 1}  \n",
       "12                                       {'n_neighbors': 1}  \n",
       "13                                  {'C': 10, 'gamma': 0.5}  \n",
       "14                                              {'C': 1000}  \n",
       "15                                              {'C': 1000}  \n",
       "16                                              {'C': 1000}  \n",
       "17                                   {'max_leaf_nodes': 50}  \n",
       "18                                   {'max_leaf_nodes': 50}  \n",
       "19                                   {'max_leaf_nodes': 50}  \n",
       "20                                   {'max_leaf_nodes': 50}  \n",
       "21                                              {'C': 1000}  \n",
       "22                        {'hidden_layer_sizes': (100, 20)}  \n",
       "23                                 {'var_smoothing': 0.001}  \n",
       "24                                 {'var_smoothing': 0.001}  \n",
       "25                                 {'var_smoothing': 0.001}  \n",
       "26                                 {'var_smoothing': 0.001}  \n",
       "27                                  {'C': 10, 'gamma': 0.5}  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalResult_df.sort_values(by='F1-Score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "w207 - final project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
